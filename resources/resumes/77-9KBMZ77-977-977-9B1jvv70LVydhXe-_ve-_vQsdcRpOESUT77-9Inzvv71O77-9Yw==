{"successResponse":"{\"document\":[{\"document\":\"\\n\\nKrunal Patel\\nData Engineer\\nkrunal.dbengg@gmail.com\\n(916) 235-3673\\n\\nProfessional Summary\\n\\n* Over 8+ years of working experience as Data Engineering with high proficient knowledge in Data\\nAnalysis and Big data.\\n* Experienced using \\\"Big data\\\" work on Hadoop, Spark, PySpark, Hive, HDFS and other NoSQL\\nplatforms.\\n* Experience in developing Map Reduce Programs using Apache Hadoop for analyzing the big data as\\nper the requirement.\\n* Experienced in Technical consulting and end-to-end delivery with architecture, data modeling,\\ndata governance and design - development - implementation of solutions.\\n* Experience in installation, configuration, supporting and managing -Cloudera Hadoop platform\\nalong with CDH4\\u0026CDH5 clusters.\\n* Strong experience and knowledge of NoSQL databases such as MongoDB and Cassandra.\\n* Proficient in Normalization/De-normalization techniques in relational/dimensional database\\nenvironments and have done normalizations up to 3NF.\\n* Hands on experience with Amazon Web Services along with provisioning and maintaining AWS\\nresources such as EMR, S3 buckets, EC2instances, RDS and others.\\n* Hands on experience with Google cloud services like GCP, BigQuery, GCS Bucket and G-Cloud\\nFunction.\\n* Experienced in Informatica ILM and Informatica Lifecycle Management and its tools.\\n* Efficient in all phases of the development lifecycle, coherent with Data Cleansing, Data\\nConversion, Data Profiling, Data Mapping, Performance Tuning and System Testing.\\n* Experience in Big Data Hadoop Ecosystem in ingestion, storage, querying, processing and\\nanalysis of Big data.\\n* Good understanding of Ralph Kimball (Dimensional) \\u0026 Bill Inman (Relational) model Methodologies\\n.\\n* Experienced working extensively on the Master Data Management(MDM) and application used for MDM.\\n* Experience in transferring the data using Informatica tool from AWS S3 to AWS Redshift.\\n* Good Knowledge on SQL queries and creating database objects like stored procedures, triggers,\\npackages and functions using SQL and PL/SQL for implementing the business techniques.\\n* Supporting ad-hoc business requests and Developed Stored Procedures and Triggers and\\nextensively used Quest tools like TOAD.\\n* Good understanding and exposure to Python programming.\\n* Excellent working experience in Scrum/Agile framework and Waterfall project execution\\nmethodologies.\\n* Experience in migrating the data using Sqoop from HDFS and Hive to Relational Database System\\nand vice-versa according to client\\u0027s requirement.\\n* Extensive experience working with business users/SMEs as well as senior management.\\n* Strong experience in using MS Excel and MS Access to dump the data and analyze based on\\nbusiness needs.\\n* Good experienced in Data Analysis as a Proficient in gathering business requirements and\\nhandling requirements management.\\n\\nTechnical Skills\\n\\n* Big Data \\u0026 Hadoop Ecosystem: MapReduce, Spark 3.3, HBase 2.3.4, Hive 2.3, Flume 1.9, Sqoop\\n1.4.6, Kafka 2.6, Oozie 4.3, Hue, Cloudera Manager, Neo4j, Hadoop 3.3, Apache NiFI 1.6\\n* Cloud Platforms: GCP, Google big-query, AWS, EC2, EC3, Redshift \\u0026 MS Azure\\n* NOSQL Database: Mongo DB, Azure Sql DB, Cassandra 3.11.10\\n* Data Modeling Tools: Erwin R9.7/9.6, ER Studio V17\\n* Databases:, Microsoft SQL Server 2017, Teradata 15.0, Oracle 12c, and MS Access\\n* BI Tools: Tableau 10, SSRS, Crystal Reports, Power BI.\\n* Programming Languages: SQL, PL/SQL, UNIX shell Scripting, R\\n* Operating Systems: Microsoft Windows Vista7/8 and 10, UNIX, and Linux.\\n* Methodologies: Agile, RAD, JAD, RUP, UML, System Development Life Cycle (SDLC), Waterfall\\nModel.\\n\\nWork Experience\\n\\nFM Global Insurance - Johnston, RI Jan 21 - Present\\nSr. Data Engineer\\nResponsibilities\\n* As a Data Engineer involved in Agile Scrum meetings to help, manage and organize a team of\\ndevelopers with regular code review sessions.\\n* Participated in Code Reviews, Enhancement discussion, maintenance of existing pipelines \\u0026\\nsystems, testing and bug-fix activities on-going basis.\\n* Worked closely with the business analysts to convert the Business Requirements into Technical\\nRequirements and prepared low and high level documentation.\\n* Worked on Spark improving the performance and optimization of the existing algorithms in Hadoop\\nusing Spark Context, Spark-SQL, Data Frame, Pair RDD\\u0027s\\n* Developed ETL Processes in AWS Glue to migrate data from external sources like S3,\\nORC/Parquet/Text\\nFiles into AWS Redshift.\\n* Worked on Ingesting data by going through cleansing and transformations and leveraging AWS Lambda\\n, AWS Glue and Step Functions.\\n* Used Spark for interactive queries, processing of streaming data and integration with popular\\nNoSQL\\ndatabase for huge volume of data.\\n* Involved in daily Scrum meetings to discuss the development/progress and was active in making\\nscrum meetings more productive.\\n* Seamlessly worked on Python to build data pipelines after the data got loaded from Kafka.\\n* Used Kafka Streams to Configure Spark Streaming to get information and then store it in HDFS.\\n* Worked on loading data into Spark RDD\\u0027s, perform advanced procedures like text analytics using\\nin-memory data computation capabilities of Spark to generate the Output response.\\n* Implemented usage of Amazon EMR for processing Big Data across a Hadoop Cluster of virtual\\nservers on Amazon Elastic Compute Cloud (EC2) and Amazon Simple Storage Service (S3)\\n* Created AWS Lambda functions and assigned IAM roles to schedule python scripts using Cloud Watch\\nTriggers to support the infrastructure needs (SQS, Event Bridge, SNS)\\n* Involved in converting MapReduce programs into Spark transformations using Spark RDD\\u0027s using Scala\\nand Python.\\n* Integrated Kafka-Spark streaming for high efficiency throughput and reliability.\\n* Developed a python script to hit REST API\\u0027s and extract data to AWS S3\\n* Conducted ETL Data Integration, Cleansing, and Transformations using AWS glue Spark script\\n* Worked on functions in Lambda that aggregates the data from incoming events, and then stored\\nresult data in Amazon Dynamo DB.\\n* Deployed the project on Amazon EMR with S3 connectivity for setting a backup storage.\\n* Designed and Developed ETL jobs to extract data from oracle and load it in data mart in Redshift\\n* Worked on AWS Data Pipeline to configure data loads from S3 to into Redshift\\n* Used JSON schema to define table and column mapping from S3 data to Redshift\\n* Connected Redshift to Tableau for creating dynamic dashboard for analytics team\\n* Used JIRA to track issues and Change Management\\n* Involved in creating Jenkins jobs for CI/CD using GIT, Maven and Bash scripting\\nEnvironment: Spark 3.3, AWS S3, Redshift, Glue, EMR, IAM, EC2, Tableau, Jenkins, Jira, Python,\\nKafka, Agile.\\n\\nAmgen - Thousand Oaks, CA Sep 18 - Dec 20\\nData Engineer\\nResponsibilities\\n* As a Data Engineer I am responsible for building scalable distributed data solutions using Hadoop\\n.\\n* Involved in Agile Development process (Scrum and Sprint planning).\\n* Handled Hadoop cluster installations in Windows environment.\\n* Migrated on-premise environment in GCP (Google Cloud Platform)\\n* Migrated data warehouses to Snowflake Data warehouse.\\n* Defined virtual warehouse sizing for Snowflake for different type of workloads.\\n* Involved in porting the existing on-premise Hive code migration to GCP (Google Cloud Platform)\\nBigQuery.\\n* Involved in migration an Oracle SQL ETL to run on Google cloud platform using cloud Dataproc \\u0026\\nBigQuery\\n, cloud pub/sub for triggering the Apache Airflow jobs.\\n* Extracted data from data lakes, EDW to relational databases for analyzing and getting more\\nmeaningful insights using SQL Queries and PySpark.\\n* Developed PySpark script to merge static and dynamic files and cleanse the data.\\n* Created Pyspark procedures, functions, packages to load data.\\n* Designed, developed and did maintenance of data integration programs in a Hadoop and RDBMS\\nenvironment with both traditional and non-traditional source systems.\\n* Developed MapReduce programs to parse the raw data, populate staging tables and store the refined\\ndata in partitioned tables in the EDW.\\n* Wrote Sqoop Scripts for importing and exporting data from RDBMS to HDFS.\\n* Set up Data Lake in Google cloud using Google cloud storage, BigQuery and Big Table.\\n* Developed scripts in BigQuery and connecting it to reporting tools.\\n* Designed workflows using Airflow to automate the services developed for Change data capture.\\n* Carried out data transformation and cleansing using SQL queries and PySpark.\\n* Used Kafka and Spark streaming to ingest real time or near real time data in HDFS.\\n* Worked related to downloading BigQuery data into Spark data frames for advanced ETL capabilities.\\n* Worked on PySpark APIs for data transformations.\\n* Built reports for monitoring data loads into GCP and drive reliability at the site level.\\n* Participated in daily stand-ups, bi-weekly scrums and PI panning.\\nEnvironment: Hadoop 3.3, GCP, BigQuery, Big Table, Spark 3.0, PySpark, Sqoop 1.4.7, ETL, HDFS,\\nSnowflake DW, Oracle Sql, MapReduce, Kafka 2.8 and Agile process.\\n\\nAll State Insurance - Boston, MA Nov 16 - Aug 18\\nSr. Data Analyst\\nResponsibilities\\n* As a Data Analyst role to review business requirement and compose source to target data mapping\\ndocuments.\\n* Interacted with Business Analyst, SMEs and other Data Engineers to understanding Business needs.\\n* Participated in design discussions and assured functional specifications are delivered in all\\nphases of SDLC in an Agile Environment.\\n* Defined appropriate security roles related to data Define roles associated with securing,\\nprovisioning.\\n* Audit security of the data within the domain of stewardship and define named individuals for each\\nrequired role.\\n* Worked closely with the business analyst and Data warehouse architect to understand the source\\ndata and need of the Warehouse.\\n* Interacted with stakeholders on clearing their doubts regarding the reports in power BI.\\n* Actively involved in SQL and Azure SQL DW code development using T-SQL\\n* Involved in designing of star schema based data model with dimensions and facts.\\n* Worked on a migration project which required gap analysis between legacy systems and new systems.\\n* Involved in requirement gathering and database design and implementation of star-schema,\\nsnowflake schema/dimensional data warehouse using Erwin.\\n* Performed and utilized necessary PL/SQL queries to analyze and validate the data.\\n* Reviewed the Joint Requirement Documents (JRD) with the cross functional team to analyze the High\\nLevel Requirements.\\n* Developed and maintained new data ingestion processes with Azure Data Factory.\\n* Implemented data aggregation and business logic in Azure Data Lake.\\n* Designed and developed automation test scripts using Python.\\n* Created publishing reports for stakeholders using power BI.\\n* Analyzed escalated incidences within the Azure SQL database.\\n* Worked on the enhancing the data quality in the database.\\n* Worked on Performance Tuning of the database which includes indexes, optimizing SQL Statements.\\n* Involved in capturing data lineage, table and column data definitions, valid values and others\\nnecessary information in the data model.\\n* Created or modified the T-SQL queries as per the business requirements.\\n* Involved in user training sessions and assisting in UAT (User Acceptance Testing).\\n* Participation in design and daily stand-up meetings.\\nEnvironment: Erwin, Azure Sql DB, Azure Data Lake, T-Sql, UAT, PL/SQL, Power BI, Python and\\nAgile/Scrum.\\n\\nNextEra Energy Inc. - Juno Beach, FL Feb 14 - Oct 16\\nData Analyst\\nResponsibilities\\n* Worked extensively along with business analysis team, scrum masters in gathering requirements and\\nunderstanding the workflows of the organization.\\n* Involved in Data mapping specifications to create and execute detailed system test plans. The\\ndata mapping specifies what data will be extracted from an internal data warehouse, transformed\\nand sent to an external entity.\\n* Analyzed business requirements, system requirements, data mapping requirement specifications, and\\nresponsible for documenting functional requirements and supplementary requirements in Quality\\nCenter.\\n* Wrote and executed unit, system, integration and UAT scripts in a data warehouse projects.\\n* Wrote and executed SQL queries to verify that data has been moved from transactional system to\\nDSS,\\nData warehouse, data mart reporting system in accordance with requirements.\\n* Created the test environment for Staging area, loading the Staging area with data from multiple\\nsources.\\n* Used and supported database applications and tools for extraction, transformation and analysis of\\nraw data.\\n* Assisted in defining business requirements for the IT team and created BRD and functional\\nspecifications documents along with mapping documents to assist the developers in their coding.\\n* Involved in building various logics to handle Slowly Changing Dimensions, Change Data Capture,\\nand Deletes for Incremental Loads in to the Data warehouse.\\n* Involved in designing fact, dimension and aggregate tables for Data warehouse Star Schema.\\n* Performed Reverse Engineering of the legacy application using DDL scripts in Erwin, and developed\\nLogical and Physical data models for Central Model consolidation\\n* Worked on data profiling and data validation to ensure the accuracy of the data between the\\nwarehouse and source systems.\\n* Monitored the Data quality of the daily processes and ensure integrity of data was maintained to\\nensure effective functioning of the departments.\\n* Developed data mapping documents for integration into a central model and depicting data flow\\nacross systems \\u0026 maintain all files into electronic filing system.\\n* Worked on SQL Server concepts SSIS (SQL Server Integration Services), SSAS (Analysis Services) and\\nSSRS (Reporting Services).\\n* Developed database objects including tables, Indexes, views, sequences, packages, triggers and\\nprocedures to troubleshoot any database problems\\n* Wrote SQL scripts to test the mappings and Developed Traceability Matrix of Business Requirements\\nmapped to Test Scripts to ensure any Change Control in requirements leads to test case update.\\n* Involved in extensive data validation by writing several complex SQL queries and Involved in\\nback-end testing and worked with data quality issues.\\n* Created or modifying the T-SQL queries as per the business requirements.\\n* Developed and optimized stored procedures for use as a data window source for complex reporting\\npurpose.\\n* Performed the batch processing of data, designed the SQL scripts, control files, batch file for\\ndata\\nloading.\\n* Delivered file in various file formatting system (ex. Excel file, Tab delimited text, Coma\\nseparated text, Pipe delimited text etc.)\\n* Performed ad hoc analyses, as needed, with the ability to comprehend analysis as needed\\nEnvironment: Sql Server 2008, SQL, XML, ad hoc, Excel 2008, data validation\\n\\n\"}],\"TK_PRESENTATION\":[{\"html\":\"\\u003cdiv\\u003e\\u003cdiv style\\u003d\\\"position: relative;\\\"\\u003e\\u003ctk_metadata name\\u003d\\\"last_modified\\\" value\\u003d\\\"\\\"\\u003e \\u003c/tk_metadata\\u003e\\u003ctk_metadata name\\u003d\\\"author\\\" value\\u003d\\\"\\\"\\u003e \\u003c/tk_metadata\\u003e \\u003cp\\u003e \\u003c/p\\u003e \\u003cp style\\u003d\\\"white-space:pre-wrap;text-align:left;margin-top:0.00in;margin-bottom:0.00in;\\\"\\u003e\\u003cspan style\\u003d\\\"font-size:12pt;\\\"\\u003eKrunal Patel\\u003cbr\\u003e \\u003c/br\\u003eData Engineer\\u003cbr\\u003e \\u003c/br\\u003ekrunal.dbengg@gmail.com\\u003cbr\\u003e \\u003c/br\\u003e(916) 235-3673\\u003cbr\\u003e \\u003c/br\\u003e\\u003cbr\\u003e \\u003c/br\\u003eProfessional Summary\\u003cbr\\u003e \\u003c/br\\u003e\\u003cbr\\u003e \\u003c/br\\u003e    *      Over 8+ years of working experience as Data Engineering with high proficient knowledge in Data\\u003cbr\\u003e \\u003c/br\\u003e       Analysis and Big data.\\u003cbr\\u003e \\u003c/br\\u003e    *      Experienced using \\u0026quot;Big data\\u0026quot; work on Hadoop, Spark, PySpark, Hive, HDFS and other NoSQL\\u003cbr\\u003e \\u003c/br\\u003e      platforms.\\u003cbr\\u003e \\u003c/br\\u003e    *      Experience in developing Map Reduce Programs using Apache Hadoop for analyzing the big data as\\u003cbr\\u003e \\u003c/br\\u003e      per the requirement.\\u003cbr\\u003e \\u003c/br\\u003e    *      Experienced in Technical consulting and end-to-end delivery with architecture, data modeling,\\u003cbr\\u003e \\u003c/br\\u003e      data governance and design - development - implementation of solutions.\\u003cbr\\u003e \\u003c/br\\u003e    *      Experience in installation, configuration, supporting and managing -Cloudera Hadoop platform\\u003cbr\\u003e \\u003c/br\\u003e      along with CDH4\\u0026amp;CDH5 clusters.\\u003cbr\\u003e \\u003c/br\\u003e    *      Strong experience and knowledge of NoSQL databases such as MongoDB and Cassandra.\\u003cbr\\u003e \\u003c/br\\u003e    *      Proficient in Normalization/De-normalization techniques in relational/dimensional database\\u003cbr\\u003e \\u003c/br\\u003e      environments and have done normalizations up to 3NF.\\u003cbr\\u003e \\u003c/br\\u003e    *      Hands on experience with Amazon Web Services along with provisioning and maintaining AWS\\u003cbr\\u003e \\u003c/br\\u003e      resources such as EMR, S3 buckets, EC2instances, RDS and others.\\u003cbr\\u003e \\u003c/br\\u003e    *      Hands on experience with Google cloud services like GCP, BigQuery, GCS Bucket and G-Cloud\\u003cbr\\u003e \\u003c/br\\u003e      Function.\\u003cbr\\u003e \\u003c/br\\u003e    *      Experienced in Informatica ILM and Informatica Lifecycle Management and its tools.\\u003cbr\\u003e \\u003c/br\\u003e    *      Efficient in all phases of the development lifecycle, coherent with Data Cleansing, Data\\u003cbr\\u003e \\u003c/br\\u003e      Conversion, Data Profiling, Data Mapping, Performance Tuning and System Testing.\\u003cbr\\u003e \\u003c/br\\u003e    *      Experience in Big Data Hadoop Ecosystem in ingestion, storage, querying, processing and\\u003cbr\\u003e \\u003c/br\\u003e      analysis of Big data.\\u003cbr\\u003e \\u003c/br\\u003e    *      Good understanding of Ralph Kimball (Dimensional) \\u0026amp; Bill Inman (Relational) model Methodologies\\u003cbr\\u003e \\u003c/br\\u003e      .\\u003cbr\\u003e \\u003c/br\\u003e    *      Experienced working extensively on the Master Data Management(MDM) and application used for MDM.\\u003cbr\\u003e \\u003c/br\\u003e    *      Experience in transferring the data using Informatica tool from AWS S3 to AWS Redshift.\\u003cbr\\u003e \\u003c/br\\u003e    *      Good Knowledge on SQL queries and creating database objects like stored procedures, triggers,\\u003cbr\\u003e \\u003c/br\\u003e      packages and functions using SQL and PL/SQL for implementing the business techniques.\\u003cbr\\u003e \\u003c/br\\u003e    *      Supporting ad-hoc business requests and Developed Stored Procedures and Triggers and\\u003cbr\\u003e \\u003c/br\\u003e      extensively used Quest tools like TOAD.\\u003cbr\\u003e \\u003c/br\\u003e    *      Good understanding and exposure to Python programming.\\u003cbr\\u003e \\u003c/br\\u003e    *      Excellent working experience in Scrum/Agile framework and Waterfall project execution\\u003cbr\\u003e \\u003c/br\\u003e      methodologies.\\u003cbr\\u003e \\u003c/br\\u003e    *      Experience in migrating the data using Sqoop from HDFS and Hive to Relational Database System\\u003cbr\\u003e \\u003c/br\\u003e      and vice-versa according to client\\u0026apos;s requirement.\\u003cbr\\u003e \\u003c/br\\u003e    *      Extensive experience working with business users/SMEs as well as senior management.\\u003cbr\\u003e \\u003c/br\\u003e    *      Strong experience in using MS Excel and MS Access to dump the data and analyze based on\\u003cbr\\u003e \\u003c/br\\u003e      business needs.\\u003cbr\\u003e \\u003c/br\\u003e    *      Good experienced in Data Analysis as a Proficient in gathering business requirements and\\u003cbr\\u003e \\u003c/br\\u003e      handling requirements management.\\u003cbr\\u003e \\u003c/br\\u003e\\u003cbr\\u003e \\u003c/br\\u003eTechnical Skills\\u003cbr\\u003e \\u003c/br\\u003e\\u003cbr\\u003e \\u003c/br\\u003e    *      Big Data \\u0026amp; Hadoop Ecosystem: MapReduce, Spark 3.3, HBase 2.3.4, Hive 2.3, Flume 1.9, Sqoop\\u003cbr\\u003e \\u003c/br\\u003e      1.4.6, Kafka 2.6, Oozie 4.3, Hue, Cloudera Manager, Neo4j, Hadoop 3.3, Apache NiFI 1.6\\u003cbr\\u003e \\u003c/br\\u003e    *      Cloud Platforms: GCP, Google big-query, AWS, EC2, EC3, Redshift \\u0026amp; MS Azure\\u003cbr\\u003e \\u003c/br\\u003e    *      NOSQL Database: Mongo DB, Azure Sql DB, Cassandra 3.11.10\\u003cbr\\u003e \\u003c/br\\u003e    *      Data Modeling Tools: Erwin R9.7/9.6, ER Studio V17\\u003cbr\\u003e \\u003c/br\\u003e    *      Databases:, Microsoft SQL Server 2017, Teradata 15.0, Oracle 12c, and MS Access\\u003cbr\\u003e \\u003c/br\\u003e    *      BI Tools: Tableau 10, SSRS, Crystal Reports, Power BI.\\u003cbr\\u003e \\u003c/br\\u003e    *      Programming Languages: SQL, PL/SQL, UNIX shell Scripting, R\\u003cbr\\u003e \\u003c/br\\u003e    *      Operating Systems: Microsoft Windows Vista7/8 and 10, UNIX, and Linux.\\u003cbr\\u003e \\u003c/br\\u003e    *      Methodologies: Agile, RAD, JAD, RUP, UML, System Development Life Cycle (SDLC), Waterfall\\u003cbr\\u003e \\u003c/br\\u003e      Model.\\u003cbr\\u003e \\u003c/br\\u003e\\u003cbr\\u003e \\u003c/br\\u003eWork Experience\\u003cbr\\u003e \\u003c/br\\u003e\\u003cbr\\u003e \\u003c/br\\u003eFM Global Insurance - Johnston, RI                                          Jan 21 - Present\\u003cbr\\u003e \\u003c/br\\u003eSr. Data Engineer\\u003cbr\\u003e \\u003c/br\\u003eResponsibilities\\u003cbr\\u003e \\u003c/br\\u003e *      As a Data Engineer involved in Agile Scrum meetings to help, manage and organize a team of\\u003cbr\\u003e \\u003c/br\\u003e    developers with regular code review sessions.\\u003cbr\\u003e \\u003c/br\\u003e *      Participated in Code Reviews, Enhancement discussion, maintenance of existing pipelines \\u0026amp;\\u003cbr\\u003e \\u003c/br\\u003e    systems, testing and bug-fix activities on-going basis.\\u003cbr\\u003e \\u003c/br\\u003e *      Worked closely with the business analysts to convert the Business Requirements into Technical\\u003cbr\\u003e \\u003c/br\\u003e    Requirements and prepared low and high level documentation.\\u003cbr\\u003e \\u003c/br\\u003e *      Worked on Spark improving the performance and optimization of the existing algorithms in Hadoop\\u003cbr\\u003e \\u003c/br\\u003e    using Spark Context, Spark-SQL, Data Frame, Pair RDD\\u0026apos;s\\u003cbr\\u003e \\u003c/br\\u003e *      Developed ETL Processes in AWS Glue to migrate data from external sources like S3, ORC/Parquet/Text\\u003cbr\\u003e \\u003c/br\\u003e     Files into AWS Redshift.\\u003cbr\\u003e \\u003c/br\\u003e *      Worked on Ingesting data by going through cleansing and transformations and leveraging AWS Lambda\\u003cbr\\u003e \\u003c/br\\u003e    , AWS Glue and Step Functions.\\u003cbr\\u003e \\u003c/br\\u003e *      Used Spark for interactive queries, processing of streaming data and integration with popular NoSQL\\u003cbr\\u003e \\u003c/br\\u003e     database for huge volume of data.\\u003cbr\\u003e \\u003c/br\\u003e *      Involved in daily Scrum meetings to discuss the development/progress and was active in making\\u003cbr\\u003e \\u003c/br\\u003e    scrum meetings more productive.\\u003cbr\\u003e \\u003c/br\\u003e *      Seamlessly worked on Python to build data pipelines after the data got loaded from Kafka.\\u003cbr\\u003e \\u003c/br\\u003e *      Used Kafka Streams to Configure Spark Streaming to get information and then store it in HDFS.\\u003cbr\\u003e \\u003c/br\\u003e *      Worked on loading data into Spark RDD\\u0026apos;s, perform advanced procedures like text analytics using\\u003cbr\\u003e \\u003c/br\\u003e    in-memory data computation capabilities of Spark to generate the Output response.\\u003cbr\\u003e \\u003c/br\\u003e *      Implemented usage of Amazon EMR for processing Big Data across a Hadoop Cluster of virtual\\u003cbr\\u003e \\u003c/br\\u003e    servers on Amazon Elastic Compute Cloud (EC2) and Amazon Simple Storage Service (S3)\\u003cbr\\u003e \\u003c/br\\u003e *      Created AWS Lambda functions and assigned IAM roles to schedule python scripts using Cloud Watch\\u003cbr\\u003e \\u003c/br\\u003e    Triggers to support the infrastructure needs (SQS, Event Bridge, SNS)\\u003cbr\\u003e \\u003c/br\\u003e *      Involved in converting MapReduce programs into Spark transformations using Spark RDD\\u0026apos;s using Scala\\u003cbr\\u003e \\u003c/br\\u003e    and Python.\\u003cbr\\u003e \\u003c/br\\u003e *      Integrated Kafka-Spark streaming for high efficiency throughput and reliability.\\u003cbr\\u003e \\u003c/br\\u003e *      Developed a python script to hit REST API\\u0026apos;s and extract data to AWS S3\\u003cbr\\u003e \\u003c/br\\u003e *      Conducted ETL Data Integration, Cleansing, and Transformations using AWS glue Spark script\\u003cbr\\u003e \\u003c/br\\u003e *      Worked on functions in Lambda that aggregates the data from incoming events, and then stored\\u003cbr\\u003e \\u003c/br\\u003e    result data in Amazon Dynamo DB.\\u003cbr\\u003e \\u003c/br\\u003e *      Deployed the project on Amazon EMR with S3 connectivity for setting a backup storage.\\u003cbr\\u003e \\u003c/br\\u003e *      Designed and Developed ETL jobs to extract data from oracle and load it in data mart in Redshift\\u003cbr\\u003e \\u003c/br\\u003e *      Worked on AWS Data Pipeline to configure data loads from S3 to into Redshift\\u003cbr\\u003e \\u003c/br\\u003e *      Used JSON schema to define table and column mapping from S3 data to Redshift\\u003cbr\\u003e \\u003c/br\\u003e *      Connected Redshift to Tableau for creating dynamic dashboard for analytics team\\u003cbr\\u003e \\u003c/br\\u003e *      Used JIRA to track issues and Change Management\\u003cbr\\u003e \\u003c/br\\u003e *      Involved in creating Jenkins jobs for CI/CD using GIT, Maven and Bash scripting\\u003cbr\\u003e \\u003c/br\\u003eEnvironment: Spark 3.3, AWS S3, Redshift, Glue, EMR, IAM, EC2, Tableau, Jenkins, Jira, Python,\\u003cbr\\u003e \\u003c/br\\u003eKafka, Agile.\\u003cbr\\u003e \\u003c/br\\u003e\\u003cbr\\u003e \\u003c/br\\u003eAmgen - Thousand Oaks, CA                                          Sep 18 - Dec 20\\u003cbr\\u003e \\u003c/br\\u003eData Engineer\\u003cbr\\u003e \\u003c/br\\u003eResponsibilities\\u003cbr\\u003e \\u003c/br\\u003e *      As a Data Engineer I am responsible for building scalable distributed data solutions using Hadoop\\u003cbr\\u003e \\u003c/br\\u003e     .\\u003cbr\\u003e \\u003c/br\\u003e *      Involved in Agile Development process (Scrum and Sprint planning).\\u003cbr\\u003e \\u003c/br\\u003e *      Handled Hadoop cluster installations in Windows environment.\\u003cbr\\u003e \\u003c/br\\u003e *      Migrated on-premise environment in GCP (Google Cloud Platform)\\u003cbr\\u003e \\u003c/br\\u003e *      Migrated data warehouses to Snowflake Data warehouse.\\u003cbr\\u003e \\u003c/br\\u003e *      Defined virtual warehouse sizing for Snowflake for different type of workloads.\\u003cbr\\u003e \\u003c/br\\u003e *      Involved in porting the existing on-premise Hive code migration to GCP (Google Cloud Platform)\\u003cbr\\u003e \\u003c/br\\u003e     BigQuery.\\u003cbr\\u003e \\u003c/br\\u003e *      Involved in migration an Oracle SQL ETL to run on Google cloud platform using cloud Dataproc \\u0026amp; BigQuery\\u003cbr\\u003e \\u003c/br\\u003e     , cloud pub/sub for triggering the Apache Airflow jobs.\\u003cbr\\u003e \\u003c/br\\u003e *      Extracted data from data lakes, EDW to relational databases for analyzing and getting more\\u003cbr\\u003e \\u003c/br\\u003e     meaningful insights using SQL Queries and PySpark.\\u003cbr\\u003e \\u003c/br\\u003e *      Developed PySpark script to merge static and dynamic files and cleanse the data.\\u003cbr\\u003e \\u003c/br\\u003e *      Created Pyspark procedures, functions, packages to load data.\\u003cbr\\u003e \\u003c/br\\u003e *      Designed, developed and did maintenance of data integration programs in a Hadoop and RDBMS\\u003cbr\\u003e \\u003c/br\\u003e     environment with both traditional and non-traditional source systems.\\u003cbr\\u003e \\u003c/br\\u003e *      Developed MapReduce programs to parse the raw data, populate staging tables and store the refined\\u003cbr\\u003e \\u003c/br\\u003e     data in partitioned tables in the EDW.\\u003cbr\\u003e \\u003c/br\\u003e *      Wrote Sqoop Scripts for importing and exporting data from RDBMS to HDFS.\\u003cbr\\u003e \\u003c/br\\u003e *      Set up Data Lake in Google cloud using Google cloud storage, BigQuery and Big Table.\\u003cbr\\u003e \\u003c/br\\u003e *      Developed scripts in BigQuery and connecting it to reporting tools.\\u003cbr\\u003e \\u003c/br\\u003e *      Designed workflows using Airflow to automate the services developed for Change data capture.\\u003cbr\\u003e \\u003c/br\\u003e *      Carried out data transformation and cleansing using SQL queries and PySpark.\\u003cbr\\u003e \\u003c/br\\u003e *      Used Kafka and Spark streaming to ingest real time or near real time data in HDFS.\\u003cbr\\u003e \\u003c/br\\u003e *      Worked related to downloading BigQuery data into Spark data frames for advanced ETL capabilities.\\u003cbr\\u003e \\u003c/br\\u003e *      Worked on PySpark APIs for data transformations.\\u003cbr\\u003e \\u003c/br\\u003e *      Built reports for monitoring data loads into GCP and drive reliability at the site level.\\u003cbr\\u003e \\u003c/br\\u003e *      Participated in daily stand-ups, bi-weekly scrums and PI panning.\\u003cbr\\u003e \\u003c/br\\u003e  Environment: Hadoop 3.3, GCP, BigQuery, Big Table, Spark 3.0, PySpark, Sqoop 1.4.7, ETL, HDFS,\\u003cbr\\u003e \\u003c/br\\u003e  Snowflake DW, Oracle Sql, MapReduce, Kafka 2.8 and Agile process.\\u003cbr\\u003e \\u003c/br\\u003e\\u003cbr\\u003e \\u003c/br\\u003eAll State Insurance - Boston, MA                                    Nov 16 - Aug 18\\u003cbr\\u003e \\u003c/br\\u003eSr. Data Analyst\\u003cbr\\u003e \\u003c/br\\u003eResponsibilities\\u003cbr\\u003e \\u003c/br\\u003e *      As a Data Analyst role to review business requirement and compose source to target data mapping\\u003cbr\\u003e \\u003c/br\\u003e      documents.\\u003cbr\\u003e \\u003c/br\\u003e *      Interacted with Business Analyst, SMEs and other Data Engineers to understanding Business needs.\\u003cbr\\u003e \\u003c/br\\u003e *      Participated in design discussions and assured functional specifications are delivered in all\\u003cbr\\u003e \\u003c/br\\u003e      phases of SDLC in an Agile Environment.\\u003cbr\\u003e \\u003c/br\\u003e *      Defined appropriate security roles related to data Define roles associated with securing,\\u003cbr\\u003e \\u003c/br\\u003e      provisioning.\\u003cbr\\u003e \\u003c/br\\u003e *      Audit security of the data within the domain of stewardship and define named individuals for each\\u003cbr\\u003e \\u003c/br\\u003e      required role.\\u003cbr\\u003e \\u003c/br\\u003e *      Worked closely with the business analyst and Data warehouse architect to understand the source\\u003cbr\\u003e \\u003c/br\\u003e      data and need of the Warehouse.\\u003cbr\\u003e \\u003c/br\\u003e *      Interacted with stakeholders on clearing their doubts regarding the reports in power BI.\\u003cbr\\u003e \\u003c/br\\u003e *      Actively involved in SQL and Azure SQL DW code development using T-SQL\\u003cbr\\u003e \\u003c/br\\u003e *      Involved in designing of star schema based data model with dimensions and facts.\\u003cbr\\u003e \\u003c/br\\u003e *      Worked on a migration project which required gap analysis between legacy systems and new systems.\\u003cbr\\u003e \\u003c/br\\u003e *      Involved in requirement gathering and database design and implementation of star-schema,\\u003cbr\\u003e \\u003c/br\\u003e      snowflake schema/dimensional data warehouse using Erwin.\\u003cbr\\u003e \\u003c/br\\u003e *      Performed and utilized necessary PL/SQL queries to analyze and validate the data.\\u003cbr\\u003e \\u003c/br\\u003e *      Reviewed the Joint Requirement Documents (JRD) with the cross functional team to analyze the High\\u003cbr\\u003e \\u003c/br\\u003e      Level Requirements.\\u003cbr\\u003e \\u003c/br\\u003e *      Developed and maintained new data ingestion processes with Azure Data Factory.\\u003cbr\\u003e \\u003c/br\\u003e *      Implemented data aggregation and business logic in Azure Data Lake.\\u003cbr\\u003e \\u003c/br\\u003e *      Designed and developed automation test scripts using Python.\\u003cbr\\u003e \\u003c/br\\u003e *      Created publishing reports for stakeholders using power BI.\\u003cbr\\u003e \\u003c/br\\u003e *      Analyzed escalated incidences within the Azure SQL database.\\u003cbr\\u003e \\u003c/br\\u003e *      Worked on the enhancing the data quality in the database.\\u003cbr\\u003e \\u003c/br\\u003e *      Worked on Performance Tuning of the database which includes indexes, optimizing SQL Statements.\\u003cbr\\u003e \\u003c/br\\u003e *      Involved in capturing data lineage, table and column data definitions, valid values and others\\u003cbr\\u003e \\u003c/br\\u003e      necessary information in the data model.\\u003cbr\\u003e \\u003c/br\\u003e *      Created or modified the T-SQL queries as per the business requirements.\\u003cbr\\u003e \\u003c/br\\u003e *      Involved in user training sessions and assisting in UAT (User Acceptance Testing).\\u003cbr\\u003e \\u003c/br\\u003e *      Participation in design and daily stand-up meetings.\\u003cbr\\u003e \\u003c/br\\u003eEnvironment: Erwin, Azure Sql DB, Azure Data Lake, T-Sql, UAT, PL/SQL, Power BI, Python and\\u003cbr\\u003e \\u003c/br\\u003eAgile/Scrum.\\u003cbr\\u003e \\u003c/br\\u003e\\u003cbr\\u003e \\u003c/br\\u003eNextEra Energy Inc. - Juno Beach, FL                                    Feb 14 - Oct 16\\u003cbr\\u003e \\u003c/br\\u003eData Analyst\\u003cbr\\u003e \\u003c/br\\u003eResponsibilities\\u003cbr\\u003e \\u003c/br\\u003e *      Worked extensively along with business analysis team, scrum masters in gathering requirements and\\u003cbr\\u003e \\u003c/br\\u003e     understanding the workflows of the organization.\\u003cbr\\u003e \\u003c/br\\u003e *      Involved in Data mapping specifications to create and execute detailed system test plans. The\\u003cbr\\u003e \\u003c/br\\u003e     data mapping specifies what data will be extracted from an internal data warehouse, transformed\\u003cbr\\u003e \\u003c/br\\u003e     and sent to an external entity.\\u003cbr\\u003e \\u003c/br\\u003e *      Analyzed business requirements, system requirements, data mapping requirement specifications, and\\u003cbr\\u003e \\u003c/br\\u003e     responsible for documenting functional requirements and supplementary requirements in Quality\\u003cbr\\u003e \\u003c/br\\u003e     Center.\\u003cbr\\u003e \\u003c/br\\u003e *      Wrote and executed unit, system, integration and UAT scripts in a data warehouse projects.\\u003cbr\\u003e \\u003c/br\\u003e *      Wrote and executed SQL queries to verify that data has been moved from transactional system to DSS,\\u003cbr\\u003e \\u003c/br\\u003e      Data warehouse, data mart reporting system in accordance with requirements.\\u003cbr\\u003e \\u003c/br\\u003e *      Created the test environment for Staging area, loading the Staging area with data from multiple\\u003cbr\\u003e \\u003c/br\\u003e     sources.\\u003cbr\\u003e \\u003c/br\\u003e *      Used and supported database applications and tools for extraction, transformation and analysis of\\u003cbr\\u003e \\u003c/br\\u003e     raw data.\\u003cbr\\u003e \\u003c/br\\u003e *      Assisted in defining business requirements for the IT team and created BRD and functional\\u003cbr\\u003e \\u003c/br\\u003e     specifications documents along with mapping documents to assist the developers in their coding.\\u003cbr\\u003e \\u003c/br\\u003e *      Involved in building various logics to handle Slowly Changing Dimensions, Change Data Capture,\\u003cbr\\u003e \\u003c/br\\u003e     and Deletes for Incremental Loads in to the Data warehouse.\\u003cbr\\u003e \\u003c/br\\u003e *      Involved in designing fact, dimension and aggregate tables for Data warehouse Star Schema.\\u003cbr\\u003e \\u003c/br\\u003e *      Performed Reverse Engineering of the legacy application using DDL scripts in Erwin, and developed\\u003cbr\\u003e \\u003c/br\\u003e     Logical and Physical data models for Central Model consolidation\\u003cbr\\u003e \\u003c/br\\u003e *      Worked on data profiling and data validation to ensure the accuracy of the data between the\\u003cbr\\u003e \\u003c/br\\u003e     warehouse and source systems.\\u003cbr\\u003e \\u003c/br\\u003e *      Monitored the Data quality of the daily processes and ensure integrity of data was maintained to\\u003cbr\\u003e \\u003c/br\\u003e     ensure effective functioning of the departments.\\u003cbr\\u003e \\u003c/br\\u003e *      Developed data mapping documents for integration into a central model and depicting data flow\\u003cbr\\u003e \\u003c/br\\u003e     across systems \\u0026amp; maintain all files into electronic filing system.\\u003cbr\\u003e \\u003c/br\\u003e *      Worked on SQL Server concepts SSIS (SQL Server Integration Services), SSAS (Analysis Services) and\\u003cbr\\u003e \\u003c/br\\u003e      SSRS (Reporting Services).\\u003cbr\\u003e \\u003c/br\\u003e *      Developed database objects including tables, Indexes, views, sequences, packages, triggers and\\u003cbr\\u003e \\u003c/br\\u003e     procedures to troubleshoot any database problems\\u003cbr\\u003e \\u003c/br\\u003e *      Wrote SQL scripts to test the mappings and Developed Traceability Matrix of Business Requirements\\u003cbr\\u003e \\u003c/br\\u003e     mapped to Test Scripts to ensure any Change Control in requirements leads to test case update.\\u003cbr\\u003e \\u003c/br\\u003e *      Involved in extensive data validation by writing several complex SQL queries and Involved in\\u003cbr\\u003e \\u003c/br\\u003e     back-end testing and worked with data quality issues.\\u003cbr\\u003e \\u003c/br\\u003e *      Created or modifying the T-SQL queries as per the business requirements.\\u003cbr\\u003e \\u003c/br\\u003e *      Developed and optimized stored procedures for use as a data window source for complex reporting\\u003cbr\\u003e \\u003c/br\\u003e     purpose.\\u003cbr\\u003e \\u003c/br\\u003e *      Performed the batch processing of data, designed the SQL scripts, control files, batch file for data\\u003cbr\\u003e \\u003c/br\\u003e      loading.\\u003cbr\\u003e \\u003c/br\\u003e *      Delivered file in various file formatting system (ex. Excel file, Tab delimited text, Coma\\u003cbr\\u003e \\u003c/br\\u003e     separated text, Pipe delimited text etc.)\\u003cbr\\u003e \\u003c/br\\u003e *      Performed ad hoc analyses, as needed, with the ability to comprehend analysis as needed\\u003cbr\\u003e \\u003c/br\\u003e  Environment: Sql Server 2008, SQL, XML, ad hoc, Excel 2008, data validation\\u003c/span\\u003e\\u003c/p\\u003e \\u003c/div\\u003e\\u003c/div\\u003e\\n\"}],\"Document\":[{\"iscv\":\"yes\"}],\"givenname\":[{\"givenname\":\"Krunal\"}],\"full_lastname\":[{\"full_lastname\":\"Patel\"}],\"gendernamedisambig\":[{\"gendernamedisambig\":\"unknown\"}],\"email\":[{\"email\":\"krunal.dbengg@gmail.com\"}],\"mobilephone\":[{\"mobilephone\":\"(916) 235-3673\"}],\"hasmanagedothers\":[{\"hasmanagedothers\":\"false\"}],\"experienceitem\":[{\"experiencecity\":\"Johnston\",\"experienceregion\":\"RI\",\"experiencecountry_english\":\"United States\",\"experience\":\"Sr. Data Engineer\",\"derived_profession_final\":\"data engineer\",\"derived_profession_description\":\"Information Engineer (m/f)\",\"derived_profession_code_id\":\"4236\",\"derived_profession_group_id\":\"86\",\"derived_profession_class_id\":\"9\",\"ansi_linked_experienceend_nowstring\":\"__NOWSTRING__\",\"experienceorg\":\"FM Global Insurance\",\"expdescriptionblock\":\"Responsibilities\\n* As a Data Engineer involved in Agile Scrum meetings to help, manage and organize a team of\\ndevelopers with regular code review sessions.\\n* Participated in Code Reviews, Enhancement discussion, maintenance of existing pipelines \\u0026\\nsystems, testing and bug-fix activities on-going basis.\\n* Worked closely with the business analysts to convert the Business Requirements into Technical\\nRequirements and prepared low and high level documentation.\\n* Worked on Spark improving the performance and optimization of the existing algorithms in Hadoop\\nusing Spark Context, Spark-SQL, Data Frame, Pair RDD\\u0027s\\n* Developed ETL Processes in AWS Glue to migrate data from external sources like S3,\\nORC/Parquet/Text\\nFiles into AWS Redshift.\\n* Worked on Ingesting data by going through cleansing and transformations and leveraging AWS Lambda\\n, AWS Glue and Step Functions.\\n* Used Spark for interactive queries, processing of streaming data and integration with popular\\nNoSQL\\ndatabase for huge volume of data.\\n* Involved in daily Scrum meetings to discuss the development/progress and was active in making\\nscrum meetings more productive.\\n* Seamlessly worked on Python to build data pipelines after the data got loaded from Kafka.\\n* Used Kafka Streams to Configure Spark Streaming to get information and then store it in HDFS.\\n* Worked on loading data into Spark RDD\\u0027s, perform advanced procedures like text analytics using\\nin-memory data computation capabilities of Spark to generate the Output response.\\n* Implemented usage of Amazon EMR for processing Big Data across a Hadoop Cluster of virtual\\nservers on Amazon Elastic Compute Cloud (EC2) and Amazon Simple Storage Service (S3)\\n* Created AWS Lambda functions and assigned IAM roles to schedule python scripts using Cloud Watch\\nTriggers to support the infrastructure needs (SQS, Event Bridge, SNS)\\n* Involved in converting MapReduce programs into Spark transformations using Spark RDD\\u0027s using Scala\\nand Python.\\n* Integrated Kafka-Spark streaming for high efficiency throughput and reliability.\\n* Developed a python script to hit REST API\\u0027s and extract data to AWS S3\\n* Conducted ETL Data Integration, Cleansing, and Transformations using AWS glue Spark script\\n* Worked on functions in Lambda that aggregates the data from incoming events, and then stored\\nresult data in Amazon Dynamo DB.\\n* Deployed the project on Amazon EMR with S3 connectivity for setting a backup storage.\\n* Designed and Developed ETL jobs to extract data from oracle and load it in data mart in Redshift\\n* Worked on AWS Data Pipeline to configure data loads from S3 to into Redshift\\n* Used JSON schema to define table and column mapping from S3 data to Redshift\\n* Connected Redshift to Tableau for creating dynamic dashboard for analytics team\\n* Used JIRA to track issues and Change Management\\n* Involved in creating Jenkins jobs for CI/CD using GIT, Maven and Bash scripting\\nEnvironment: Spark 3.3, AWS S3, Redshift, Glue, EMR, IAM, EC2, Tableau, Jenkins, Jira, Python,\\nKafka, Agile.\",\"jobtype\":\"fulltime\",\"iscurrentitem\":\"1\",\"experienceitem\":\"FM Global Insurance - Johnston, RI Jan 21 - Present Sr. Data Engineer Responsibilities * As a Data Engineer involved in Agile Scrum meetings to help, manage and organize a team of developers with regular code review sessions. * Participated in Code Reviews, Enhancement discussion, maintenance of existing pipelines \\u0026 systems, testing and bug-fix activities on-going basis. * Worked closely with the business analysts to convert the Business Requirements into Technical Requirements and prepared low and high level documentation. * Worked on Spark improving the performance and optimization of the existing algorithms in Hadoop using Spark Context, Spark-SQL, Data Frame, Pair RDD\\u0027s * Developed ETL Processes in AWS Glue to migrate data from external sources like S3, ORC/Parquet/Text Files into AWS Redshift. * Worked on Ingesting data by going through cleansing and transformations and leveraging AWS Lambda , AWS Glue and Step Functions. * Used Spark for interactive queries, processing of streaming data and integration with popular NoSQL database for huge volume of data. * Involved in daily Scrum meetings to discuss the development/progress and was active in making scrum meetings more productive. * Seamlessly worked on Python to build data pipelines after the data got loaded from Kafka. * Used Kafka Streams to Configure Spark Streaming to get information and then store it in HDFS. * Worked on loading data into Spark RDD\\u0027s, perform advanced procedures like text analytics using in-memory data computation capabilities of Spark to generate the Output response. * Implemented usage of Amazon EMR for processing Big Data across a Hadoop Cluster of virtual servers on Amazon Elastic Compute Cloud (EC2) and Amazon Simple Storage Service (S3) * Created AWS Lambda functions and assigned IAM roles to schedule python scripts using Cloud Watch Triggers to support the infrastructure needs (SQS, Event Bridge, SNS) * Involved in converting MapReduce programs into Spark transformations using Spark RDD\\u0027s using Scala and Python. * Integrated Kafka-Spark streaming for high efficiency throughput and reliability. * Developed a python script to hit REST API\\u0027s and extract data to AWS S3 * Conducted ETL Data Integration, Cleansing, and Transformations using AWS glue Spark script * Worked on functions in Lambda that aggregates the data from incoming events, and then stored result data in Amazon Dynamo DB. * Deployed the project on Amazon EMR with S3 connectivity for setting a backup storage. * Designed and Developed ETL jobs to extract data from oracle and load it in data mart in Redshift * Worked on AWS Data Pipeline to configure data loads from S3 to into Redshift * Used JSON schema to define table and column mapping from S3 data to Redshift * Connected Redshift to Tableau for creating dynamic dashboard for analytics team * Used JIRA to track issues and Change Management * Involved in creating Jenkins jobs for CI/CD using GIT, Maven and Bash scripting Environment: Spark 3.3, AWS S3, Redshift, Glue, EMR, IAM, EC2, Tableau, Jenkins, Jira, Python, Kafka, Agile.\",\"experiencemonths\":\"3\"},{\"experiencecity\":\"Thousand Oaks\",\"experienceregion\":\"CA\",\"experiencecountry_english\":\"United States\",\"experience\":\"Data Engineer\",\"derived_profession_final\":\"data engineer\",\"derived_profession_description\":\"Information Engineer (m/f)\",\"derived_profession_code_id\":\"4236\",\"derived_profession_group_id\":\"86\",\"derived_profession_class_id\":\"9\",\"ansi_linked_experiencebegin_nowstring\":\"2018-09-01\",\"experienceorg\":\"Amgen\",\"expdescriptionblock\":\"Responsibilities\\n* As a Data Engineer I am responsible for building scalable distributed data solutions using Hadoop\\n.\\n* Involved in Agile Development process (Scrum and Sprint planning).\\n* Handled Hadoop cluster installations in Windows environment.\\n* Migrated on-premise environment in GCP (Google Cloud Platform)\\n* Migrated data warehouses to Snowflake Data warehouse.\\n* Defined virtual warehouse sizing for Snowflake for different type of workloads.\\n* Involved in porting the existing on-premise Hive code migration to GCP (Google Cloud Platform)\\nBigQuery.\\n* Involved in migration an Oracle SQL ETL to run on Google cloud platform using cloud Dataproc \\u0026\\nBigQuery\\n, cloud pub/sub for triggering the Apache Airflow jobs.\\n* Extracted data from data lakes, EDW to relational databases for analyzing and getting more\\nmeaningful insights using SQL Queries and PySpark.\\n* Developed PySpark script to merge static and dynamic files and cleanse the data.\\n* Created Pyspark procedures, functions, packages to load data.\\n* Designed, developed and did maintenance of data integration programs in a Hadoop and RDBMS\\nenvironment with both traditional and non-traditional source systems.\\n* Developed MapReduce programs to parse the raw data, populate staging tables and store the refined\\ndata in partitioned tables in the EDW.\\n* Wrote Sqoop Scripts for importing and exporting data from RDBMS to HDFS.\\n* Set up Data Lake in Google cloud using Google cloud storage, BigQuery and Big Table.\\n* Developed scripts in BigQuery and connecting it to reporting tools.\\n* Designed workflows using Airflow to automate the services developed for Change data capture.\\n* Carried out data transformation and cleansing using SQL queries and PySpark.\\n* Used Kafka and Spark streaming to ingest real time or near real time data in HDFS.\\n* Worked related to downloading BigQuery data into Spark data frames for advanced ETL capabilities.\\n* Worked on PySpark APIs for data transformations.\\n* Built reports for monitoring data loads into GCP and drive reliability at the site level.\\n* Participated in daily stand-ups, bi-weekly scrums and PI panning.\\nEnvironment: Hadoop 3.3, GCP, BigQuery, Big Table, Spark 3.0, PySpark, Sqoop 1.4.7, ETL, HDFS,\\nSnowflake DW, Oracle Sql, MapReduce, Kafka 2.8 and Agile process.\",\"jobtype\":\"fulltime\",\"experienceitem\":\"Amgen - Thousand Oaks, CA Sep 18 - Dec 20 Data Engineer Responsibilities * As a Data Engineer I am responsible for building scalable distributed data solutions using Hadoop . * Involved in Agile Development process (Scrum and Sprint planning). * Handled Hadoop cluster installations in Windows environment. * Migrated on-premise environment in GCP (Google Cloud Platform) * Migrated data warehouses to Snowflake Data warehouse. * Defined virtual warehouse sizing for Snowflake for different type of workloads. * Involved in porting the existing on-premise Hive code migration to GCP (Google Cloud Platform) BigQuery. * Involved in migration an Oracle SQL ETL to run on Google cloud platform using cloud Dataproc \\u0026 BigQuery , cloud pub/sub for triggering the Apache Airflow jobs. * Extracted data from data lakes, EDW to relational databases for analyzing and getting more meaningful insights using SQL Queries and PySpark. * Developed PySpark script to merge static and dynamic files and cleanse the data. * Created Pyspark procedures, functions, packages to load data. * Designed, developed and did maintenance of data integration programs in a Hadoop and RDBMS environment with both traditional and non-traditional source systems. * Developed MapReduce programs to parse the raw data, populate staging tables and store the refined data in partitioned tables in the EDW. * Wrote Sqoop Scripts for importing and exporting data from RDBMS to HDFS. * Set up Data Lake in Google cloud using Google cloud storage, BigQuery and Big Table. * Developed scripts in BigQuery and connecting it to reporting tools. * Designed workflows using Airflow to automate the services developed for Change data capture. * Carried out data transformation and cleansing using SQL queries and PySpark. * Used Kafka and Spark streaming to ingest real time or near real time data in HDFS. * Worked related to downloading BigQuery data into Spark data frames for advanced ETL capabilities. * Worked on PySpark APIs for data transformations. * Built reports for monitoring data loads into GCP and drive reliability at the site level. * Participated in daily stand-ups, bi-weekly scrums and PI panning. Environment: Hadoop 3.3, GCP, BigQuery, Big Table, Spark 3.0, PySpark, Sqoop 1.4.7, ETL, HDFS, Snowflake DW, Oracle Sql, MapReduce, Kafka 2.8 and Agile process.\",\"experiencemonths\":\"4\"},{\"experiencecity\":\"Boston\",\"experienceregion\":\"MA\",\"experiencecountry_english\":\"United States\",\"experience\":\"Sr. Data Analyst\",\"derived_profession_final\":\"data analyst\",\"derived_profession_description\":\"Data Analyst (m/f)\",\"derived_profession_code_id\":\"2627\",\"derived_profession_group_id\":\"86\",\"derived_profession_class_id\":\"9\",\"ansi_linked_experiencebegin_nowstring\":\"2016-11-01\",\"ansi_linked_experienceend_nowstring\":\"2018-08-31\",\"experienceorg\":\"All State Insurance\",\"expdescriptionblock\":\"Responsibilities\\n* As a Data Analyst role to review business requirement and compose source to target data mapping\\ndocuments.\\n* Interacted with Business Analyst, SMEs and other Data Engineers to understanding Business needs.\\n* Participated in design discussions and assured functional specifications are delivered in all\\nphases of SDLC in an Agile Environment.\\n* Defined appropriate security roles related to data Define roles associated with securing,\\nprovisioning.\\n* Audit security of the data within the domain of stewardship and define named individuals for each\\nrequired role.\\n* Worked closely with the business analyst and Data warehouse architect to understand the source\\ndata and need of the Warehouse.\\n* Interacted with stakeholders on clearing their doubts regarding the reports in power BI.\\n* Actively involved in SQL and Azure SQL DW code development using T-SQL\\n* Involved in designing of star schema based data model with dimensions and facts.\\n* Worked on a migration project which required gap analysis between legacy systems and new systems.\\n* Involved in requirement gathering and database design and implementation of star-schema,\\nsnowflake schema/dimensional data warehouse using Erwin.\\n* Performed and utilized necessary PL/SQL queries to analyze and validate the data.\\n* Reviewed the Joint Requirement Documents (JRD) with the cross functional team to analyze the High\\nLevel Requirements.\\n* Developed and maintained new data ingestion processes with Azure Data Factory.\\n* Implemented data aggregation and business logic in Azure Data Lake.\\n* Designed and developed automation test scripts using Python.\\n* Created publishing reports for stakeholders using power BI.\\n* Analyzed escalated incidences within the Azure SQL database.\\n* Worked on the enhancing the data quality in the database.\\n* Worked on Performance Tuning of the database which includes indexes, optimizing SQL Statements.\\n* Involved in capturing data lineage, table and column data definitions, valid values and others\\nnecessary information in the data model.\\n* Created or modified the T-SQL queries as per the business requirements.\\n* Involved in user training sessions and assisting in UAT (User Acceptance Testing).\\n* Participation in design and daily stand-up meetings.\\nEnvironment: Erwin, Azure Sql DB, Azure Data Lake, T-Sql, UAT, PL/SQL, Power BI, Python and\\nAgile/Scrum.\",\"jobtype\":\"fulltime\",\"experienceitem\":\"All State Insurance - Boston, MA Nov 16 - Aug 18 Sr. Data Analyst Responsibilities * As a Data Analyst role to review business requirement and compose source to target data mapping documents. * Interacted with Business Analyst, SMEs and other Data Engineers to understanding Business needs. * Participated in design discussions and assured functional specifications are delivered in all phases of SDLC in an Agile Environment. * Defined appropriate security roles related to data Define roles associated with securing, provisioning. * Audit security of the data within the domain of stewardship and define named individuals for each required role. * Worked closely with the business analyst and Data warehouse architect to understand the source data and need of the Warehouse. * Interacted with stakeholders on clearing their doubts regarding the reports in power BI. * Actively involved in SQL and Azure SQL DW code development using T-SQL * Involved in designing of star schema based data model with dimensions and facts. * Worked on a migration project which required gap analysis between legacy systems and new systems. * Involved in requirement gathering and database design and implementation of star-schema, snowflake schema/dimensional data warehouse using Erwin. * Performed and utilized necessary PL/SQL queries to analyze and validate the data. * Reviewed the Joint Requirement Documents (JRD) with the cross functional team to analyze the High Level Requirements. * Developed and maintained new data ingestion processes with Azure Data Factory. * Implemented data aggregation and business logic in Azure Data Lake. * Designed and developed automation test scripts using Python. * Created publishing reports for stakeholders using power BI. * Analyzed escalated incidences within the Azure SQL database. * Worked on the enhancing the data quality in the database. * Worked on Performance Tuning of the database which includes indexes, optimizing SQL Statements. * Involved in capturing data lineage, table and column data definitions, valid values and others necessary information in the data model. * Created or modified the T-SQL queries as per the business requirements. * Involved in user training sessions and assisting in UAT (User Acceptance Testing). * Participation in design and daily stand-up meetings. Environment: Erwin, Azure Sql DB, Azure Data Lake, T-Sql, UAT, PL/SQL, Power BI, Python and Agile/Scrum.\",\"experiencemonths\":\"22\"},{\"experiencecity\":\"Juno Beach\",\"experienceregion\":\"FL\",\"experiencecountry_english\":\"United States\",\"experience\":\"Data Analyst\",\"derived_profession_final\":\"data analyst\",\"derived_profession_description\":\"Data Analyst (m/f)\",\"derived_profession_code_id\":\"2627\",\"derived_profession_group_id\":\"86\",\"derived_profession_class_id\":\"9\",\"ansi_linked_experiencebegin_nowstring\":\"2014-02-01\",\"ansi_linked_experienceend_nowstring\":\"2016-10-31\",\"experienceorg\":\"NextEra Energy Inc.\",\"expdescriptionblock\":\"Responsibilities\\n* Worked extensively along with business analysis team, scrum masters in gathering requirements and\\nunderstanding the workflows of the organization.\\n* Involved in Data mapping specifications to create and execute detailed system test plans. The\\ndata mapping specifies what data will be extracted from an internal data warehouse, transformed\\nand sent to an external entity.\\n* Analyzed business requirements, system requirements, data mapping requirement specifications, and\\nresponsible for documenting functional requirements and supplementary requirements in Quality\\nCenter.\\n* Wrote and executed unit, system, integration and UAT scripts in a data warehouse projects.\\n* Wrote and executed SQL queries to verify that data has been moved from transactional system to\\nDSS,\\nData warehouse, data mart reporting system in accordance with requirements.\\n* Created the test environment for Staging area, loading the Staging area with data from multiple\\nsources.\\n* Used and supported database applications and tools for extraction, transformation and analysis of\\nraw data.\\n* Assisted in defining business requirements for the IT team and created BRD and functional\\nspecifications documents along with mapping documents to assist the developers in their coding.\\n* Involved in building various logics to handle Slowly Changing Dimensions, Change Data Capture,\\nand Deletes for Incremental Loads in to the Data warehouse.\\n* Involved in designing fact, dimension and aggregate tables for Data warehouse Star Schema.\\n* Performed Reverse Engineering of the legacy application using DDL scripts in Erwin, and developed\\nLogical and Physical data models for Central Model consolidation\\n* Worked on data profiling and data validation to ensure the accuracy of the data between the\\nwarehouse and source systems.\\n* Monitored the Data quality of the daily processes and ensure integrity of data was maintained to\\nensure effective functioning of the departments.\\n* Developed data mapping documents for integration into a central model and depicting data flow\\nacross systems \\u0026 maintain all files into electronic filing system.\\n* Worked on SQL Server concepts SSIS (SQL Server Integration Services), SSAS (Analysis Services) and\\nSSRS (Reporting Services).\\n* Developed database objects including tables, Indexes, views, sequences, packages, triggers and\\nprocedures to troubleshoot any database problems\\n* Wrote SQL scripts to test the mappings and Developed Traceability Matrix of Business Requirements\\nmapped to Test Scripts to ensure any Change Control in requirements leads to test case update.\\n* Involved in extensive data validation by writing several complex SQL queries and Involved in\\nback-end testing and worked with data quality issues.\\n* Created or modifying the T-SQL queries as per the business requirements.\\n* Developed and optimized stored procedures for use as a data window source for complex reporting\\npurpose.\\n* Performed the batch processing of data, designed the SQL scripts, control files, batch file for\\ndata\\nloading.\\n* Delivered file in various file formatting system (ex. Excel file, Tab delimited text, Coma\\nseparated text, Pipe delimited text etc.)\\n* Performed ad hoc analyses, as needed, with the ability to comprehend analysis as needed\\nEnvironment: Sql Server 2008, SQL, XML, ad hoc, Excel 2008, data validation\",\"jobtype\":\"fulltime\",\"experienceitem\":\"NextEra Energy Inc. - Juno Beach, FL Feb 14 - Oct 16 Data Analyst Responsibilities * Worked extensively along with business analysis team, scrum masters in gathering requirements and understanding the workflows of the organization. * Involved in Data mapping specifications to create and execute detailed system test plans. The data mapping specifies what data will be extracted from an internal data warehouse, transformed and sent to an external entity. * Analyzed business requirements, system requirements, data mapping requirement specifications, and responsible for documenting functional requirements and supplementary requirements in Quality Center. * Wrote and executed unit, system, integration and UAT scripts in a data warehouse projects. * Wrote and executed SQL queries to verify that data has been moved from transactional system to DSS, Data warehouse, data mart reporting system in accordance with requirements. * Created the test environment for Staging area, loading the Staging area with data from multiple sources. * Used and supported database applications and tools for extraction, transformation and analysis of raw data. * Assisted in defining business requirements for the IT team and created BRD and functional specifications documents along with mapping documents to assist the developers in their coding. * Involved in building various logics to handle Slowly Changing Dimensions, Change Data Capture, and Deletes for Incremental Loads in to the Data warehouse. * Involved in designing fact, dimension and aggregate tables for Data warehouse Star Schema. * Performed Reverse Engineering of the legacy application using DDL scripts in Erwin, and developed Logical and Physical data models for Central Model consolidation * Worked on data profiling and data validation to ensure the accuracy of the data between the warehouse and source systems. * Monitored the Data quality of the daily processes and ensure integrity of data was maintained to ensure effective functioning of the departments. * Developed data mapping documents for integration into a central model and depicting data flow across systems \\u0026 maintain all files into electronic filing system. * Worked on SQL Server concepts SSIS (SQL Server Integration Services), SSAS (Analysis Services) and SSRS (Reporting Services). * Developed database objects including tables, Indexes, views, sequences, packages, triggers and procedures to troubleshoot any database problems * Wrote SQL scripts to test the mappings and Developed Traceability Matrix of Business Requirements mapped to Test Scripts to ensure any Change Control in requirements leads to test case update. * Involved in extensive data validation by writing several complex SQL queries and Involved in back-end testing and worked with data quality issues. * Created or modifying the T-SQL queries as per the business requirements. * Developed and optimized stored procedures for use as a data window source for complex reporting purpose. * Performed the batch processing of data, designed the SQL scripts, control files, batch file for data loading. * Delivered file in various file formatting system (ex. Excel file, Tab delimited text, Coma separated text, Pipe delimited text etc.) * Performed ad hoc analyses, as needed, with the ability to comprehend analysis as needed Environment: Sql Server 2008, SQL, XML, ad hoc, Excel 2008, data validation\",\"experiencemonths\":\"33\"}],\"totalexperiencemonths\":[{\"totalexperiencemonths\":\"62\"}],\"lastitemwithjobtitle\":[{\"experiencecity\":\"Johnston\",\"experienceregion\":\"RI\",\"experiencecountry_english\":\"United States\",\"experience\":\"Sr. Data Engineer\",\"derived_profession_final\":\"data engineer\",\"derived_profession_description\":\"Information Engineer (m/f)\",\"derived_profession_code_id\":\"4236\",\"derived_profession_group_id\":\"86\",\"derived_profession_class_id\":\"9\",\"ansi_linked_experienceend_nowstring\":\"__NOWSTRING__\",\"experienceorg\":\"FM Global Insurance\",\"expdescriptionblock\":\"Responsibilities\\n* As a Data Engineer involved in Agile Scrum meetings to help, manage and organize a team of\\ndevelopers with regular code review sessions.\\n* Participated in Code Reviews, Enhancement discussion, maintenance of existing pipelines \\u0026\\nsystems, testing and bug-fix activities on-going basis.\\n* Worked closely with the business analysts to convert the Business Requirements into Technical\\nRequirements and prepared low and high level documentation.\\n* Worked on Spark improving the performance and optimization of the existing algorithms in Hadoop\\nusing Spark Context, Spark-SQL, Data Frame, Pair RDD\\u0027s\\n* Developed ETL Processes in AWS Glue to migrate data from external sources like S3,\\nORC/Parquet/Text\\nFiles into AWS Redshift.\\n* Worked on Ingesting data by going through cleansing and transformations and leveraging AWS Lambda\\n, AWS Glue and Step Functions.\\n* Used Spark for interactive queries, processing of streaming data and integration with popular\\nNoSQL\\ndatabase for huge volume of data.\\n* Involved in daily Scrum meetings to discuss the development/progress and was active in making\\nscrum meetings more productive.\\n* Seamlessly worked on Python to build data pipelines after the data got loaded from Kafka.\\n* Used Kafka Streams to Configure Spark Streaming to get information and then store it in HDFS.\\n* Worked on loading data into Spark RDD\\u0027s, perform advanced procedures like text analytics using\\nin-memory data computation capabilities of Spark to generate the Output response.\\n* Implemented usage of Amazon EMR for processing Big Data across a Hadoop Cluster of virtual\\nservers on Amazon Elastic Compute Cloud (EC2) and Amazon Simple Storage Service (S3)\\n* Created AWS Lambda functions and assigned IAM roles to schedule python scripts using Cloud Watch\\nTriggers to support the infrastructure needs (SQS, Event Bridge, SNS)\\n* Involved in converting MapReduce programs into Spark transformations using Spark RDD\\u0027s using Scala\\nand Python.\\n* Integrated Kafka-Spark streaming for high efficiency throughput and reliability.\\n* Developed a python script to hit REST API\\u0027s and extract data to AWS S3\\n* Conducted ETL Data Integration, Cleansing, and Transformations using AWS glue Spark script\\n* Worked on functions in Lambda that aggregates the data from incoming events, and then stored\\nresult data in Amazon Dynamo DB.\\n* Deployed the project on Amazon EMR with S3 connectivity for setting a backup storage.\\n* Designed and Developed ETL jobs to extract data from oracle and load it in data mart in Redshift\\n* Worked on AWS Data Pipeline to configure data loads from S3 to into Redshift\\n* Used JSON schema to define table and column mapping from S3 data to Redshift\\n* Connected Redshift to Tableau for creating dynamic dashboard for analytics team\\n* Used JIRA to track issues and Change Management\\n* Involved in creating Jenkins jobs for CI/CD using GIT, Maven and Bash scripting\\nEnvironment: Spark 3.3, AWS S3, Redshift, Glue, EMR, IAM, EC2, Tableau, Jenkins, Jira, Python,\\nKafka, Agile.\",\"experienceitem\":\"FM Global Insurance - Johnston, RI Jan 21 - Present Sr. Data Engineer Responsibilities * As a Data Engineer involved in Agile Scrum meetings to help, manage and organize a team of developers with regular code review sessions. * Participated in Code Reviews, Enhancement discussion, maintenance of existing pipelines \\u0026 systems, testing and bug-fix activities on-going basis. * Worked closely with the business analysts to convert the Business Requirements into Technical Requirements and prepared low and high level documentation. * Worked on Spark improving the performance and optimization of the existing algorithms in Hadoop using Spark Context, Spark-SQL, Data Frame, Pair RDD\\u0027s * Developed ETL Processes in AWS Glue to migrate data from external sources like S3, ORC/Parquet/Text Files into AWS Redshift. * Worked on Ingesting data by going through cleansing and transformations and leveraging AWS Lambda , AWS Glue and Step Functions. * Used Spark for interactive queries, processing of streaming data and integration with popular NoSQL database for huge volume of data. * Involved in daily Scrum meetings to discuss the development/progress and was active in making scrum meetings more productive. * Seamlessly worked on Python to build data pipelines after the data got loaded from Kafka. * Used Kafka Streams to Configure Spark Streaming to get information and then store it in HDFS. * Worked on loading data into Spark RDD\\u0027s, perform advanced procedures like text analytics using in-memory data computation capabilities of Spark to generate the Output response. * Implemented usage of Amazon EMR for processing Big Data across a Hadoop Cluster of virtual servers on Amazon Elastic Compute Cloud (EC2) and Amazon Simple Storage Service (S3) * Created AWS Lambda functions and assigned IAM roles to schedule python scripts using Cloud Watch Triggers to support the infrastructure needs (SQS, Event Bridge, SNS) * Involved in converting MapReduce programs into Spark transformations using Spark RDD\\u0027s using Scala and Python. * Integrated Kafka-Spark streaming for high efficiency throughput and reliability. * Developed a python script to hit REST API\\u0027s and extract data to AWS S3 * Conducted ETL Data Integration, Cleansing, and Transformations using AWS glue Spark script * Worked on functions in Lambda that aggregates the data from incoming events, and then stored result data in Amazon Dynamo DB. * Deployed the project on Amazon EMR with S3 connectivity for setting a backup storage. * Designed and Developed ETL jobs to extract data from oracle and load it in data mart in Redshift * Worked on AWS Data Pipeline to configure data loads from S3 to into Redshift * Used JSON schema to define table and column mapping from S3 data to Redshift * Connected Redshift to Tableau for creating dynamic dashboard for analytics team * Used JIRA to track issues and Change Management * Involved in creating Jenkins jobs for CI/CD using GIT, Maven and Bash scripting Environment: Spark 3.3, AWS S3, Redshift, Glue, EMR, IAM, EC2, Tableau, Jenkins, Jira, Python, Kafka, Agile.\",\"experiencemonths\":\"3\"}],\"validated_compskill\":[{\"validated_compskill\":\"Cassandra\"},{\"validated_compskill\":\"EC2instances\"},{\"validated_compskill\":\"GCS\"},{\"validated_compskill\":\"Flume 1.9\"},{\"validated_compskill\":\"EC3\"},{\"ansi_linked_last_used_nowstring\":\"__NOWSTRING__\",\"duration_derived_months\":\"3\",\"validated_compskill\":\"NOSQL\"},{\"ansi_linked_first_used_nowstring\":\"2016-11-01\",\"ansi_linked_last_used_nowstring\":\"2018-08-31\",\"duration_derived_months\":\"22\",\"validated_compskill\":\"Power BI\"},{\"validated_compskill\":\"UNIX shell Scripting\"},{\"validated_compskill\":\"Agile\"},{\"ansi_linked_first_used_nowstring\":\"2016-11-01\",\"ansi_linked_last_used_nowstring\":\"__NOWSTRING__\",\"duration_derived_months\":\"25\",\"validated_compskill\":\"Python\"},{\"ansi_linked_last_used_nowstring\":\"__NOWSTRING__\",\"duration_derived_months\":\"3\",\"validated_compskill\":\"Data Pipeline\"},{\"ansi_linked_last_used_nowstring\":\"__NOWSTRING__\",\"duration_derived_months\":\"3\",\"validated_compskill\":\"Bash scripting\"},{\"ansi_linked_first_used_nowstring\":\"2018-09-01\",\"duration_derived_months\":\"4\",\"validated_compskill\":\"Windows environment\"},{\"ansi_linked_first_used_nowstring\":\"2018-09-01\",\"duration_derived_months\":\"4\",\"validated_compskill\":\"Apache\"},{\"ansi_linked_first_used_nowstring\":\"2014-02-01\",\"ansi_linked_last_used_nowstring\":\"2018-08-31\",\"duration_derived_months\":\"59\",\"validated_compskill\":\"SQL Queries\"},{\"ansi_linked_first_used_nowstring\":\"2018-09-01\",\"duration_derived_months\":\"4\",\"validated_compskill\":\"RDBMS\"},{\"ansi_linked_first_used_nowstring\":\"2018-09-01\",\"duration_derived_months\":\"4\",\"validated_compskill\":\"data capture\"},{\"ansi_linked_first_used_nowstring\":\"2018-09-01\",\"duration_derived_months\":\"4\",\"validated_compskill\":\"APIs\"},{\"ansi_linked_first_used_nowstring\":\"2014-02-01\",\"ansi_linked_last_used_nowstring\":\"2018-08-31\",\"duration_derived_months\":\"55\",\"validated_compskill\":\"star schema\"},{\"ansi_linked_first_used_nowstring\":\"2014-02-01\",\"ansi_linked_last_used_nowstring\":\"2016-10-31\",\"duration_derived_months\":\"33\",\"validated_compskill\":\"data profiling\"}],\"langskill\":[{\"langskill\":\"English\",\"langskill_code\":\"EN\"}],\"language\":[{\"language\":\"english\"}]}","service":"TK_RESUME","insertionTime":1647548007795}
{"successResponse":"{\"document\":[{\"document\":\"\\nSiddharth Aerva\\n469-310-3476\\nakshitek-siddharth@yahoo.com\\n\\nSUMMARY\\n    *      71/2 years of professional experience in design, development and implementations of robust\\n     technology systems, with specialized expertise in Hadoop Administration, Big Data and Linux\\n     Administration.\\n    *      Experience in using various Hadoop infrastructures such as Map Reduce, Pig, Hive, Zookeeper,\\n     HBase, Sqoop, YARN, Spark, Kafka, Oozie, and Flume for data storage and analysis.\\n    *      Experience in deploying and managing the Hadoop cluster using Cloudera Manager and Apache\\n     Ambari.\\n    *      Installed and configured various Hadoop distributions like CDH and HDP.\\n    *      Involved in Hadoop Cluster environment administration that includes adding and removing\\n     cluster nodes, cluster capacity planning, performance tuning, cluster monitoring,\\n     Troubleshooting.\\n    *      Supporting Hadoop developers and assisting in optimization of map reduce jobs, Hive Scripts\\n     and HBase ingest required.\\n    *      Collected logs of data from various sources and integrated into HDFS Using Flume and Sqoop.\\n    *      Experienced in running MapReduce and Spark jobs over YARN.\\n    *      Excellent understanding of Hadoop Cluster security and implemented secure Hadoop cluster using\\n     Kerberos.\\n    *      Setting up automated 24x7 monitoring and escalation infrastructure for Hadoop cluster using\\n     Nagios.\\n    *      Experience in using NOSQL databases like HBase and Cassandra.\\n    *      Experience in Sentry, Ranger, Knox configuration to provide the security for Hadoop components\\n     depending on distribution tool.\\n    *      Good experience on Design, configure and manage the backup and disaster recovery for Hadoop\\n     data.\\n    *      Hands on experience in analyzing Log files for Hadoop and eco system services and finding root\\n     cause.\\n    *      Experience in understanding the security requirements for Hadoop and integrating with Kerberos\\n     authentication infrastructure- KDC server setup, creating realm /domain, managing principals.\\n    *      Experience in using automation tools like puppet and chef.\\n    *      Experience in setting up of Hadoop cluster in cloud services like AWS and GCP.\\n    *      Knowledge on AWS services such as EC2, S3, Glaciers, IAM, EBS, SNS, SQS, RDS, VPC, Load\\n     Balancers, Auto scaling, Cloud Formation, Cloud Front and Cloud Watch.\\n    *      Experience in Linux System Administration, Linux System Security, Project Management and Risk\\n     Management in Information Systems.\\n    *      Involved in the functional usage and deployment of applications to Oracle WebLogic, JBOSS,\\n     Apache Tomcat, Nginx and WebSphere servers.\\n    *      Worked on Monitoring Tool Nagios for Resource Monitoring/Network Monitoring/Log Trace\\n     Monitoring.\\n    *      Experience on working with VMware Workstation and Virtual Box.\\n    *      Capable of managing multiple projects simultaneously, comfortable troubleshooting and\\n     debugging and able to work under pressure.\\n    *      Excellent communicative, interpersonal, intuitive, analysis and leadership skills with ability\\n     to work efficiently in both independent and team work environments.\\n    *      Experience in requirements gathering, analysis, solution design, development, implementation,\\n     setup, testing, customization, maintenance, and support and data migration.\\n\\nTECHNICAL SKILLS\\nHadoop/ Big Data    HDFS, MapReduce, Yarn, Pig, Hive, Sqoop, Flume, Kafka, Spark, Solr, Oozie, Zookeeper, CDH, HDP,\\nAmbari, Cloudera Manager, Kerberos\\nLanguages    C, C++, Java, HTML, JavaScript\\nScripting    Shell Script, Powershell, Python\\nOperating System    Linux/Unix (Redhat (6,7), Ubuntu (12.04,14.04), OEL (6,7), Centos), Windows XP, Windows 8.1, 10\\nVirtualization    VMware, Virtual Box, Vagrant\\nDatabases    Oracle 10g, PL/SQL, MySql, PostgreSQL, HBase, Cassandra\\nDevops/Cloud Tools    AWS, Azure, Chef, Puppet\\nMonitoring Tools    Nagios, Splunk\\nOthers    Apache Tomcat, Weblogic, WebSphere, Tableau\\n\\n\\nEDUCATION\\n    *      Bachelor of Technology in Computer Science.\\n\\nPROFESSIONAL EXPERIENCE\\n\\nHadoop Administrator\\n84.51, Cincinnati, OH      Feb 2019 - Present\\n84.51 is a consumer insights subsidiary to Kroger. It is majorly a data science analytics company\\nwhose applications help in building strong relationship between people and brands.\\nResponsibilities:\\n    *      Administering and managing large-scale Hadoop clusters (6 Pb Storage) and also part of\\n     capacity planning team for optimizing/scaling the clusters for future needs.\\n    *      Worked on different Hadoop clusters running on Cloudera and Hortonworks distributions.\\n    *      Responsible for day-to-day activities which include Hadoop support, Cluster maintenance,\\n     handling nodes, and Monitoring/ Troubleshooting clusters.\\n    *      Supported and configured Pyspark jobs through Jupyter sessions and R scripts through RStudio.\\n    *      Installed various python and R packages.\\n    *      Implemented high availability, DR strategies, load balancers for all the Hadoop services along\\n     with Cloudera manager to overcome single point of failure.\\n    *      Worked with fellow developers in compressing the data, modifying Spark application code with\\n     repartition, coalesce methods in order to reduce the small files and storage on cluster side.\\n    *      Installed and configured Unravel (Application Performance Management tool) to optimize the\\n     resources and run the applications effectively.\\n    *      Used/Configured Nifi Flows on Hortonworks platform for data ingestion purposes and also to\\n     export data from hdfs to Exadata.\\n    *      Worked on setting up Kafka to integrate with Nifi to ingest the live streaming data.\\n    *      Worked with Cloud Team (GCP and later Azure) to migrate hdfs data into cloud.\\n    *      Automated various Day to Day works using shell and python scripts.\\n    *      Oozie workflows, Airflow jobs, shell/python scripts were setup for effective Cluster\\n     Monitoring and to run applications.\\n    *      Integrated both Hadoop clusters and Cloud Infrastructure with Grafana and Dynatrace for\\n     performance monitoring.\\n    *      Secured the data/user access with ACL\\u0027s, Kerberos, Sentry and Encryption Zones.\\nEnvironment: Hadoop, Hdfs, Spark, MapReduce, Yarn, Hive, Sqoop, Oozie, Airflow, Nifi, Kafka,\\nKerberos, Shell Scripting, Unravel.\\n\\nHadoop Administrator\\nMasterCard, O\\u0027Fallon, MO      Oct 2017 - Jan 2019\\nMastercard is the leading global payments \\u0026 technology company connecting businesses, consumers,\\nmerchants, issuers \\u0026 governments around the world. Its global operations headquarters is located in\\nO\\u0027fallon, Missouri.\\nResponsibilities:\\n    *      Installing and Configuring Hadoop ecosystem (HDFS/Spark/Hive/Yarn/Kafka) using Cloudera\\n     manager and securing the cluster with Kerberos and encryption servers (KTS \\u0026KMS)\\n    *      Worked on administration and management of large-scale Hadoop clusters (300 nodes) and was\\n     part of capacity planning team in scaling these clusters for future needs.\\n    *      Responsible for day-to-day activities which include Hadoop support, Cluster maintenance,\\n     creation/removal of nodes, Cluster Monitoring/ Troubleshooting, Manage and review Hadoop log\\n     files, Backup restoring and capacity planning.\\n    *      Worked with the vendor Infoworks for Data migration between 2 large-scale hadoop clusters.\\n    *      Created DR for both production clusters and scheduled replication jobs to copy data between\\n     the clusters.\\n    *      Made backups and Recovered data using HDFS snapshots.\\n    *      Configured the HUE and CM SSL on the cluster using certificates.\\n    *      Involved in setting up the KAFKA MirrorMaker on the production clusters to make sure the same\\n     set of streaming data reaching production cluster is replicated to DR cluster.\\n    *      Configured Kafka Brokers to receive all the streaming data and then connected it to Flume to\\n     send data to Hbase and Hdfs.\\n    *      Worked with Informatica team in connecting Informatica Stand Alone Servers with the cluster\\n     metadata for visualizing the data and generate reports.\\n    *      Created an exclusive hadoop cluster for Informatica and also made sure GDPR norms being\\n     followed.\\n    *      Various Spark jobs were run to refine the streaming data and store only the refined data in\\n     hdfs.\\n    *      Enabled security to the cluster using Kerberos and integrated clusters with LDAP/AD at\\n     Enterprise level.\\n    *      Secured the directories with ACL\\u0027s and Encryption Zones.\\n    *      Restricted the User Access on the data using Sentry by providing adequate read/write\\n     permissions.\\n    *      Extensively worked on different versions of Oracle Enterprise Linux (OEL).\\n    *      Handled the OS upgrade project from OEL 6 to 7 on all hadoop nodes with minor cluster\\n     downtimes.\\n    *      Configured the cluster to use PostgreSQL database for storage.\\n    *      Created and maintained various Shell scripts for automating various processes.\\n    *      Installed and configured Flume, Hive, Hbase, Sqoop, Impala and Oozie on the hadoop cluster.\\n    *      Worked with developers and data analysts in running hive and Impala queries and even\\n     configured the resources needed for those jobs.\\n    *      Configured various service resource parameters in the cluster to make sure jobs run quicker\\n     and doesn\\u0027t fail in the cluster.\\n    *      Worked on POC to automate the configuration/building of hadoop cluster using Chef and also\\n     running hadoop jobs using Docker.\\n    *      Set up the Hadoop cluster on Google Cloud Platform (GCP) using cloudera director.\\n    *      Experience with Splunk Administration, ran splunk queries to gather all the hadoop/server data\\nEnvironment: Hadoop, Hdfs, Spark, MapReduce, Yarn, Hive, Sqoop, Oozie, Kafka, Solr, OEL, PostgreSQL,\\nHBase, Kerberos, Shell Scripting, Chef, Docker.\\n\\nHadoop/Cloud Administrator\\nBank of The West, San Francisco, CA      Jan 2016 - Sep 2017\\nBank of the West is a diversified financial service holding company, headquartered in San Francisco,\\nCalifornia. This bank has more than 700 banking centers in the Midwest and Western United States.\\nResponsibilities:\\n    *      Installing and Configuring Hadoop ecosystem (HDFS/Spark/Hive/Oozie/Yarn) using Cloudera\\n     manager and CDH.\\n    *      Monitored job performances, file system/disk-space management, cluster \\u0026 database\\n     connectivity, log files, management of backup/security and troubleshooting various user issues.\\n    *      Imported and Exported the analyzed data to the relational databases using Sqoop for\\n     visualization and to generate reports for the BI team.\\n    *      Experience in setting up Dynamic Resource pools for distributing resources between pools.\\n    *      Implemented the workflows using Apache Oozie framework to automate tasks and Developed Job\\n     Processing scripts using Oozie Workflow.\\n    *      Used Spark streaming to receive real time data from the Kafka and store the stream data to\\n     HDFS using Scala and NoSql databases such as HBase and Cassandra.\\n    *      Responsible for Performance Tuning of Spark Applications for setting right Batch Interval\\n     time, correct level of Parallelism and Memory tuning.\\n    *      Installed and Configured Hive and Pig. Worked with developers to develop various Hive and\\n     PigLatin scripts.\\n    *      Involved in implementing High Availability and automatic failover infrastructure to overcome\\n     single point of failure for Name node utilizing zookeeper services.\\n    *      Experience in setting up Hadoop clusters on cloud platforms like AWS.\\n    *      Worked with teams in setting up AWS EC2 instances by using different AWS services like S3,\\n     EBS, Elastic Load Balancer, and Auto scaling groups, IAM roles, VPC subnets and CloudWatch.\\n    *      Used Nagios to monitor the cluster to receive alerts around the clock.\\n    *      Experience with Splunk Administration, Add-On\\u0027s, Dashboards, Clustering and Forwarder\\n     Management.\\n    *      Enabled security to the cluster using Kerberos and integrated clusters with LDAP/AD at\\n     Enterprise level.\\n    *      Extensively worked on Linux systems (RHEL/CentOS).\\n    *      Worked with different file formats such as Text, Sequence files, Avro, ORC and Parquet.\\nEnvironment: Hadoop, Hdfs, Spark, MapReduce, Yarn, Pig, Hive, Sqoop, Oozie, Kafka, Linux, AWS,\\nHBase, Cassandra, Kerberos, Scala, Shell Scripting.\\n\\n\\nBig Data Engineer\\nTMobile, Bellevue, WA                                                         Sep 2014 - Dec 2015\\nAs America\\u0027s Un-carrier, T-Mobile US, Inc. (NASDAQ: TMUS) is redefining the way consumers and\\nbusinesses buy wireless services through leading product and service innovation.\\nResponsibilities:\\n    *      Installed and Configured Hortonworks Data Platform (HDP) and Apache Ambari.\\n    *      Installed and Configured Hadoop Ecosystem (MapReduce, Pig, Sqoop. Hive, Kafka) both manually\\n     and using Ambari Server.\\n    *      Supported in setting up QA environment and updating configurations for implementing scripts\\n     with Pig and Sqoop. Worked on tuning the performance Pig queries.\\n    *      Converted ETL operations to Hadoop system using Pig Latin Operations, transformations and\\n     functions.\\n    *      Capturing data from existing databases that provide SQL interfaces using Sqoop.\\n    *      Worked on YARN capacity scheduler by creating queues to allocate resource guarantee to\\n     specific groups.\\n    *      Responsible for adding new eco system components, like spark, flume, Knox with required custom\\n     configurations based on the requirements.\\n    *      Installed and configured Kafka Cluster with additional zookeeper service.\\n    *      Helped the team to increase cluster size. The configuration for additional data nodes was\\n     managed using Puppet manifests.\\n    *      knowledge of open source system monitoring and event handling tools like Nagios.\\n    *      Integrated BI and Analytical tools like Tableau, Business Objects, and SAS with Hadoop\\n     Cluster.\\n    *      Worked with Ranger, Knox configuration to provide centralized security to Hadoop services.\\n    *      Expertise with NoSQL databases like Hbase and Cassandra.\\n    *      Planning and implementation of data migration from existing staging to production cluster.\\n     Even migrated data from existing databases to cloud (S3 and AWS RDS).\\n    *      Analyze escalated incidences within the Azure SQL database. Implemented test scripts to\\n     support test driven development and continuous integration.\\n    *      Developed Python and Shell/Perl Scripts for automation purpose.\\n    *      Troubleshooting experience in debugging and fixed the wrong data or data missing problem for\\n     Oracle Database.\\nEnvironment: Hadoop, Hdfs, Spark, MapReduce, Yarn, Pig, Hive, Sqoop, Kafka, Linux, Azure, HBase,\\nCassandra, MySQL, Kerberos, Python, Shell Scripting, Tableau, SAS.\\n\\nHadoop Engineer\\nReply Inc., Chicago, IL                                          Jun 2013 - Aug 2014\\nReply is a company that specializes in consulting, system integration \\u0026 digital services, with a\\nfocus on the design and implementation of solutions based on the web and social networks. It\\nutilizes a network model consisting of companies operating in different sectors, including big data,\\ncloud computing, digital media and the internet of things.\\nResponsibilities:\\n    *      Installed/Configured Hadoop Map Reduce, HDFS and developed Map Reduce jobs in Java for data\\n     cleaning and processing.\\n    *      Responsible for troubleshooting issues in the execution of Map Reduce jobs by inspecting and\\n     reviewing log files.\\n    *      Implemented and Configured High Availability Hadoop Cluster (Quorum Based).\\n    *      Worked on Ingesting data into HDFS using Sqoop and Flume.\\n    *      Loaded/ Transformed large sets of structured, semi structured and unstructured data.\\n    *      Worked on different databases such as Oracle and MySql.\\n    *      Developed Oozie Workflows for daily incremental loads, which gets data from Teradata and then\\n     imported into hive tables.\\n    *      Used Cloudera manager to pull metrics on various cluster features like JVM, Running Map and\\n     reduce tasks.\\n    *      Developed PIG Latin scripts to extract the data from the web server output files to load into\\n     HDFS\\n    *      Performed upgrades and configuration changes, Commissioned/decommission nodes as needed to\\n     maintain load balancing and latency on the servers.\\n    *      Maintained DNS, NFS, and DHCP, printing, mail, web, and FTP services for the enterprise.\\n    *      Administered VMware and Virtual Box environment, evaluated and implemented new hardware and\\n     software.\\n    *      Worked on NGINX for proxy and reverse proxy rewrite rules.\\n    *      Worked on the Oracle databases in the backend to execute the DMLs and DDLs.\\nEnvironment: Hadoop, HDFS, MapReduce, Sqoop, Hive, Flume, MySQL, MR, HBase, PIG Latin, Sqoop, Chef,\\nOracle db,\\n\\nBig Data Analyst\\nApplied Materials, Austin, TX                                                Dec 2012 - May 2013\\nApplied Materials, Inc. is an American corporation that supplies equipment, services and software to\\nenable the manufacture of semiconductor (integrated circuit) chips for electronics, flat panel\\ndisplays for computers, smart phones and televisions, and solar products. In order to process their\\nhuge data, they started using Hadoop.\\nResponsibilities:\\n    *      Participated in the review of functional and non-functional requirements.\\n    *      Designed and implemented Map Reduce jobs to analyze the data collected by the Flume server.\\n    *      Worked with Hadoop Administration team to debug various slow running MR jobs and perform the\\n     necessary optimizations.\\n    *      Monitored cluster health status on daily basis, tuning system performance related\\n     configuration parameters, backing up configuration XML files.\\n    *      Worked with network and Linux system engineers/admin to define optimum network configurations,\\n     server hardware and operating system.\\n    *      Developed and implemented APIs to retrieve the data from Hadoop Platform to Web Application.\\n    *      Created Hive tables and worked on them using Hive QL.\\n    *      Performed Bug fixing of various modules that were raised by the Testing teams in the\\n     application during the Integration testing phase.\\n    *      Facilitated knowledge transfer sessions.\\nEnvironment: Hadoop, Cloudera CDH, Java, Eclipse, Pig, Hive, Ubuntu, Shell Scripting.\\n\\nSystem Analyst/Admin\\nDwaith Infotech, India       Dec 2011 - Nov 2012\\nDwaith Infotech, a leading IT Outsourcing, IT Consulting firm offering managed IT services,\\nEnterprise Business Solutions and Collaborative client partnerships through its flexible Global\\nDelivery model. Dwaith focus on select industries and service lines, enabling us to deliver the best\\npossible solutions to meet our clients\\u0027 needs.\\nResponsibilities:\\n    *      Have been responsible for administering large, multi-site UNIX/LINUX server environments and\\n     operating systems, software installation, upgrades, system integrity, security, disaster\\n     recovery and performance.\\n    *      Implemented Cloudera Hadoop clusters for three different environments on HPE ProLiant servers.\\n    *      Configured/Managed Cisco and Brocade Fabric environment for soft/hard Zoning.\\n    *      Worked on MySQL and successfully launched queries to provide required data to the department.\\n    *      Created users, manage user permissions, maintain User \\u0026 File System quota on Redhat Linux.\\n    *      Installation \\u0026 Configuration of Logical Volume Manager - LVM and RAID.\\n    *      Automated administration tasks through use of scripting and Job Scheduling using CRON.\\n    *      Wrote shell scripts for taking data backups, cleaning junk content and updating software\\n     regularly.\\n    *      Experience in using protocols/services like Http, Https, TLI/SSL, DHCP, DNS, SSH, SFTP,\\n     TCP/IP, FTP/SFTP, SMTP\\n    *      Provided Linux System Administration, Linux System Security, Project Management and Risk\\n     Management in Information Systems.\\n    *      Day to day provisioning of storage including (Storage device/LUN/Volume selection \\u0026 creation,\\n     Fabric Zoning, LUN Masking \\u0026 Mapping).\\n    *      Administration of environment running VMware ESXi Hosts and Virtual Machines.\\n    *      Worked successfully towards improving and maintaining the Backup Success rate to \\u003e 98%.\\n    *      Worked with server teams to insure the configuration and installation of the proper drivers,\\n     firmware, and multipath drivers to support the SAN environment.\\n    *      Provide performance tuning and regular maintenance in order to minimize downtime and maximize\\n     performance.\\n    *      Take care about Data Center (DC) by ordering and upgrading necessary hardware, supporting\\n     RAIDs, maintaining servers and installing new ones.\\nEnvironment: Linux, VMware, Storage\"}],\"Document\":[{\"iscv\":\"yes\"}],\"givenname\":[{\"givenname\":\"Siddharth\"}],\"full_lastname\":[{\"full_lastname\":\"Aerva\"}],\"gendernamedisambig\":[{\"gendernamedisambig\":\"male\"}],\"email\":[{\"email\":\"akshitek-siddharth@yahoo.com\"}],\"mobilephone\":[{\"mobilephone\":\"469-310-3476\"}],\"hasmanagedothers\":[{\"hasmanagedothers\":\"false\"}],\"experienceitem\":[{\"experiencecity\":\"Cincinnati\",\"experienceregion\":\"OH\",\"experiencecountry_english\":\"United States\",\"experience\":\"Hadoop Administrator\",\"derived_profession_final\":\"hadoop administrator\",\"derived_profession_description\":\"Data Architect (m/f)\",\"derived_profession_code_id\":\"3126\",\"derived_profession_group_id\":\"79\",\"derived_profession_class_id\":\"9\",\"ansi_linked_experiencebegin_nowstring\":\"1984-05-01\",\"expdescriptionblock\":\"84.51 is a consumer insights subsidiary to Kroger. It is majorly a data science analytics company\\nwhose applications help in building strong relationship between people and brands.\\nResponsibilities:\\n    *      Administering and managing large-scale Hadoop clusters (6 Pb Storage) and also part of\\n     capacity planning team for optimizing/scaling the clusters for future needs.\\n    *      Worked on different Hadoop clusters running on Cloudera and Hortonworks distributions.\\n    *      Responsible for day-to-day activities which include Hadoop support, Cluster maintenance,\\n     handling nodes, and Monitoring/ Troubleshooting clusters.\\n    *      Supported and configured Pyspark jobs through Jupyter sessions and R scripts through RStudio.\\n    *      Installed various python and R packages.\\n    *      Implemented high availability, DR strategies, load balancers for all the Hadoop services along\\n     with Cloudera manager to overcome single point of failure.\\n    *      Worked with fellow developers in compressing the data, modifying Spark application code with\\n     repartition, coalesce methods in order to reduce the small files and storage on cluster side.\\n    *      Installed and configured Unravel (Application Performance Management tool) to optimize the\\n     resources and run the applications effectively.\\n    *      Used/Configured Nifi Flows on Hortonworks platform for data ingestion purposes and also to\\n     export data from hdfs to Exadata.\\n    *      Worked on setting up Kafka to integrate with Nifi to ingest the live streaming data.\\n    *      Worked with Cloud Team (GCP and later Azure) to migrate hdfs data into cloud.\\n    *      Automated various Day to Day works using shell and python scripts.\\n    *      Oozie workflows, Airflow jobs, shell/python scripts were setup for effective Cluster\\n     Monitoring and to run applications.\\n    *      Integrated both Hadoop clusters and Cloud Infrastructure with Grafana and Dynatrace for\\n     performance monitoring.\\n    *      Secured the data/user access with ACL\\u0027s, Kerberos, Sentry and Encryption Zones.\\nEnvironment: Hadoop, Hdfs, Spark, MapReduce, Yarn, Hive, Sqoop, Oozie, Airflow, Nifi, Kafka,\\nKerberos, Shell Scripting, Unravel.\",\"jobtype\":\"fulltime\",\"experienceitem\":\"Hadoop Administrator 84.51, Cincinnati, OH Feb 2019 - Present 84.51 is a consumer insights subsidiary to Kroger. It is majorly a data science analytics company whose applications help in building strong relationship between people and brands. Responsibilities: * Administering and managing large-scale Hadoop clusters (6 Pb Storage) and also part of capacity planning team for optimizing/scaling the clusters for future needs. * Worked on different Hadoop clusters running on Cloudera and Hortonworks distributions. * Responsible for day-to-day activities which include Hadoop support, Cluster maintenance, handling nodes, and Monitoring/ Troubleshooting clusters. * Supported and configured Pyspark jobs through Jupyter sessions and R scripts through RStudio. * Installed various python and R packages. * Implemented high availability, DR strategies, load balancers for all the Hadoop services along with Cloudera manager to overcome single point of failure. * Worked with fellow developers in compressing the data, modifying Spark application code with repartition, coalesce methods in order to reduce the small files and storage on cluster side. * Installed and configured Unravel (Application Performance Management tool) to optimize the resources and run the applications effectively. * Used/Configured Nifi Flows on Hortonworks platform for data ingestion purposes and also to export data from hdfs to Exadata. * Worked on setting up Kafka to integrate with Nifi to ingest the live streaming data. * Worked with Cloud Team (GCP and later Azure) to migrate hdfs data into cloud. * Automated various Day to Day works using shell and python scripts. * Oozie workflows, Airflow jobs, shell/python scripts were setup for effective Cluster Monitoring and to run applications. * Integrated both Hadoop clusters and Cloud Infrastructure with Grafana and Dynatrace for performance monitoring. * Secured the data/user access with ACL\\u0027s, Kerberos, Sentry and Encryption Zones. Environment: Hadoop, Hdfs, Spark, MapReduce, Yarn, Hive, Sqoop, Oozie, Airflow, Nifi, Kafka, Kerberos, Shell Scripting, Unravel.\",\"experiencemonths\":\"8\"},{\"experiencecity\":\"O\\u0027Fallon\",\"experienceregion\":\"MO\",\"experiencecountry_english\":\"United States\",\"experience\":\"Hadoop Administrator\",\"derived_profession_final\":\"hadoop administrator\",\"derived_profession_description\":\"Data Architect (m/f)\",\"derived_profession_code_id\":\"3126\",\"derived_profession_group_id\":\"79\",\"derived_profession_class_id\":\"9\",\"ansi_linked_experiencebegin_nowstring\":\"2017-10-01\",\"ansi_linked_experienceend_nowstring\":\"2019-01-31\",\"experienceorg\":\"MasterCard\",\"expdescriptionblock\":\"Mastercard is the leading global payments \\u0026 technology company connecting businesses, consumers,\\nmerchants, issuers \\u0026 governments around the world. Its global operations headquarters is located in\\nO\\u0027fallon, Missouri.\\nResponsibilities:\\n    *      Installing and Configuring Hadoop ecosystem (HDFS/Spark/Hive/Yarn/Kafka) using Cloudera\\n     manager and securing the cluster with Kerberos and encryption servers (KTS \\u0026KMS)\\n    *      Worked on administration and management of large-scale Hadoop clusters (300 nodes) and was\\n     part of capacity planning team in scaling these clusters for future needs.\\n    *      Responsible for day-to-day activities which include Hadoop support, Cluster maintenance,\\n     creation/removal of nodes, Cluster Monitoring/ Troubleshooting, Manage and review Hadoop log\\n     files, Backup restoring and capacity planning.\\n    *      Worked with the vendor Infoworks for Data migration between 2 large-scale hadoop clusters.\\n    *      Created DR for both production clusters and scheduled replication jobs to copy data between\\n     the clusters.\\n    *      Made backups and Recovered data using HDFS snapshots.\\n    *      Configured the HUE and CM SSL on the cluster using certificates.\\n    *      Involved in setting up the KAFKA MirrorMaker on the production clusters to make sure the same\\n     set of streaming data reaching production cluster is replicated to DR cluster.\\n    *      Configured Kafka Brokers to receive all the streaming data and then connected it to Flume to\\n     send data to Hbase and Hdfs.\\n    *      Worked with Informatica team in connecting Informatica Stand Alone Servers with the cluster\\n     metadata for visualizing the data and generate reports.\\n    *      Created an exclusive hadoop cluster for Informatica and also made sure GDPR norms being\\n     followed.\\n    *      Various Spark jobs were run to refine the streaming data and store only the refined data in\\n     hdfs.\\n    *      Enabled security to the cluster using Kerberos and integrated clusters with LDAP/AD at\\n     Enterprise level.\\n    *      Secured the directories with ACL\\u0027s and Encryption Zones.\\n    *      Restricted the User Access on the data using Sentry by providing adequate read/write\\n     permissions.\\n    *      Extensively worked on different versions of Oracle Enterprise Linux (OEL).\\n    *      Handled the OS upgrade project from OEL 6 to 7 on all hadoop nodes with minor cluster\\n     downtimes.\\n    *      Configured the cluster to use PostgreSQL database for storage.\\n    *      Created and maintained various Shell scripts for automating various processes.\\n    *      Installed and configured Flume, Hive, Hbase, Sqoop, Impala and Oozie on the hadoop cluster.\\n    *      Worked with developers and data analysts in running hive and Impala queries and even\\n     configured the resources needed for those jobs.\\n    *      Configured various service resource parameters in the cluster to make sure jobs run quicker\\n     and doesn\\u0027t fail in the cluster.\\n    *      Worked on POC to automate the configuration/building of hadoop cluster using Chef and also\\n     running hadoop jobs using Docker.\\n    *      Set up the Hadoop cluster on Google Cloud Platform (GCP) using cloudera director.\\n    *      Experience with Splunk Administration, ran splunk queries to gather all the hadoop/server data\\nEnvironment: Hadoop, Hdfs, Spark, MapReduce, Yarn, Hive, Sqoop, Oozie, Kafka, Solr, OEL, PostgreSQL,\\nHBase, Kerberos, Shell Scripting, Chef, Docker.\",\"jobtype\":\"fulltime\",\"experienceitem\":\"Hadoop Administrator MasterCard, O\\u0027Fallon, MO Oct 2017 - Jan 2019 Mastercard is the leading global payments \\u0026 technology company connecting businesses, consumers, merchants, issuers \\u0026 governments around the world. Its global operations headquarters is located in O\\u0027fallon, Missouri. Responsibilities: * Installing and Configuring Hadoop ecosystem (HDFS/Spark/Hive/Yarn/Kafka) using Cloudera manager and securing the cluster with Kerberos and encryption servers (KTS \\u0026KMS) * Worked on administration and management of large-scale Hadoop clusters (300 nodes) and was part of capacity planning team in scaling these clusters for future needs. * Responsible for day-to-day activities which include Hadoop support, Cluster maintenance, creation/removal of nodes, Cluster Monitoring/ Troubleshooting, Manage and review Hadoop log files, Backup restoring and capacity planning. * Worked with the vendor Infoworks for Data migration between 2 large-scale hadoop clusters. * Created DR for both production clusters and scheduled replication jobs to copy data between the clusters. * Made backups and Recovered data using HDFS snapshots. * Configured the HUE and CM SSL on the cluster using certificates. * Involved in setting up the KAFKA MirrorMaker on the production clusters to make sure the same set of streaming data reaching production cluster is replicated to DR cluster. * Configured Kafka Brokers to receive all the streaming data and then connected it to Flume to send data to Hbase and Hdfs. * Worked with Informatica team in connecting Informatica Stand Alone Servers with the cluster metadata for visualizing the data and generate reports. * Created an exclusive hadoop cluster for Informatica and also made sure GDPR norms being followed. * Various Spark jobs were run to refine the streaming data and store only the refined data in hdfs. * Enabled security to the cluster using Kerberos and integrated clusters with LDAP/AD at Enterprise level. * Secured the directories with ACL\\u0027s and Encryption Zones. * Restricted the User Access on the data using Sentry by providing adequate read/write permissions. * Extensively worked on different versions of Oracle Enterprise Linux (OEL). * Handled the OS upgrade project from OEL 6 to 7 on all hadoop nodes with minor cluster downtimes. * Configured the cluster to use PostgreSQL database for storage. * Created and maintained various Shell scripts for automating various processes. * Installed and configured Flume, Hive, Hbase, Sqoop, Impala and Oozie on the hadoop cluster. * Worked with developers and data analysts in running hive and Impala queries and even configured the resources needed for those jobs. * Configured various service resource parameters in the cluster to make sure jobs run quicker and doesn\\u0027t fail in the cluster. * Worked on POC to automate the configuration/building of hadoop cluster using Chef and also running hadoop jobs using Docker. * Set up the Hadoop cluster on Google Cloud Platform (GCP) using cloudera director. * Experience with Splunk Administration, ran splunk queries to gather all the hadoop/server data Environment: Hadoop, Hdfs, Spark, MapReduce, Yarn, Hive, Sqoop, Oozie, Kafka, Solr, OEL, PostgreSQL, HBase, Kerberos, Shell Scripting, Chef, Docker.\",\"experiencemonths\":\"16\"},{\"experiencecity\":\"San Francisco\",\"experienceregion\":\"CA\",\"experiencecountry_english\":\"United States\",\"experience\":\"Hadoop/Cloud Administrator\",\"derived_profession_final\":\"hadoop cloud administrator\",\"derived_profession_description\":\"Data Architect (m/f)\",\"derived_profession_code_id\":\"3126\",\"derived_profession_group_id\":\"79\",\"derived_profession_class_id\":\"9\",\"ansi_linked_experiencebegin_nowstring\":\"2016-01-01\",\"ansi_linked_experienceend_nowstring\":\"2017-09-30\",\"experienceorg\":\"Bank of The West\",\"expdescriptionblock\":\"Bank of the West is a diversified financial service holding company, headquartered in San Francisco,\\nCalifornia. This bank has more than 700 banking centers in the Midwest and Western United States.\\nResponsibilities:\\n    *      Installing and Configuring Hadoop ecosystem (HDFS/Spark/Hive/Oozie/Yarn) using Cloudera\\n     manager and CDH.\\n    *      Monitored job performances, file system/disk-space management, cluster \\u0026 database\\n     connectivity, log files, management of backup/security and troubleshooting various user issues.\\n    *      Imported and Exported the analyzed data to the relational databases using Sqoop for\\n     visualization and to generate reports for the BI team.\\n    *      Experience in setting up Dynamic Resource pools for distributing resources between pools.\\n    *      Implemented the workflows using Apache Oozie framework to automate tasks and Developed Job\\n     Processing scripts using Oozie Workflow.\\n    *      Used Spark streaming to receive real time data from the Kafka and store the stream data to\\n     HDFS using Scala and NoSql databases such as HBase and Cassandra.\\n    *      Responsible for Performance Tuning of Spark Applications for setting right Batch Interval\\n     time, correct level of Parallelism and Memory tuning.\\n    *      Installed and Configured Hive and Pig. Worked with developers to develop various Hive and\\n     PigLatin scripts.\\n    *      Involved in implementing High Availability and automatic failover infrastructure to overcome\\n     single point of failure for Name node utilizing zookeeper services.\\n    *      Experience in setting up Hadoop clusters on cloud platforms like AWS.\\n    *      Worked with teams in setting up AWS EC2 instances by using different AWS services like S3,\\n     EBS, Elastic Load Balancer, and Auto scaling groups, IAM roles, VPC subnets and CloudWatch.\\n    *      Used Nagios to monitor the cluster to receive alerts around the clock.\\n    *      Experience with Splunk Administration, Add-On\\u0027s, Dashboards, Clustering and Forwarder\\n     Management.\\n    *      Enabled security to the cluster using Kerberos and integrated clusters with LDAP/AD at\\n     Enterprise level.\\n    *      Extensively worked on Linux systems (RHEL/CentOS).\\n    *      Worked with different file formats such as Text, Sequence files, Avro, ORC and Parquet.\\nEnvironment: Hadoop, Hdfs, Spark, MapReduce, Yarn, Pig, Hive, Sqoop, Oozie, Kafka, Linux, AWS,\\nHBase, Cassandra, Kerberos, Scala, Shell Scripting.\",\"jobtype\":\"fulltime\",\"experienceitem\":\"Hadoop/Cloud Administrator Bank of The West, San Francisco, CA Jan 2016 - Sep 2017 Bank of the West is a diversified financial service holding company, headquartered in San Francisco, California. This bank has more than 700 banking centers in the Midwest and Western United States. Responsibilities: * Installing and Configuring Hadoop ecosystem (HDFS/Spark/Hive/Oozie/Yarn) using Cloudera manager and CDH. * Monitored job performances, file system/disk-space management, cluster \\u0026 database connectivity, log files, management of backup/security and troubleshooting various user issues. * Imported and Exported the analyzed data to the relational databases using Sqoop for visualization and to generate reports for the BI team. * Experience in setting up Dynamic Resource pools for distributing resources between pools. * Implemented the workflows using Apache Oozie framework to automate tasks and Developed Job Processing scripts using Oozie Workflow. * Used Spark streaming to receive real time data from the Kafka and store the stream data to HDFS using Scala and NoSql databases such as HBase and Cassandra. * Responsible for Performance Tuning of Spark Applications for setting right Batch Interval time, correct level of Parallelism and Memory tuning. * Installed and Configured Hive and Pig. Worked with developers to develop various Hive and PigLatin scripts. * Involved in implementing High Availability and automatic failover infrastructure to overcome single point of failure for Name node utilizing zookeeper services. * Experience in setting up Hadoop clusters on cloud platforms like AWS. * Worked with teams in setting up AWS EC2 instances by using different AWS services like S3, EBS, Elastic Load Balancer, and Auto scaling groups, IAM roles, VPC subnets and CloudWatch. * Used Nagios to monitor the cluster to receive alerts around the clock. * Experience with Splunk Administration, Add-On\\u0027s, Dashboards, Clustering and Forwarder Management. * Enabled security to the cluster using Kerberos and integrated clusters with LDAP/AD at Enterprise level. * Extensively worked on Linux systems (RHEL/CentOS). * Worked with different file formats such as Text, Sequence files, Avro, ORC and Parquet. Environment: Hadoop, Hdfs, Spark, MapReduce, Yarn, Pig, Hive, Sqoop, Oozie, Kafka, Linux, AWS, HBase, Cassandra, Kerberos, Scala, Shell Scripting.\",\"experiencemonths\":\"21\"},{\"experiencecity\":\"Bellevue\",\"experienceregion\":\"WA\",\"experiencecountry_english\":\"United States\",\"experience\":\"Big Data Engineer\",\"derived_profession_final\":\"big data engineer\",\"derived_profession_description\":\"Information Engineer (m/f)\",\"derived_profession_code_id\":\"4236\",\"derived_profession_group_id\":\"86\",\"derived_profession_class_id\":\"9\",\"ansi_linked_experiencebegin_nowstring\":\"2014-09-01\",\"ansi_linked_experienceend_nowstring\":\"2015-12-31\",\"experienceorg\":\"TMobile\",\"expdescriptionblock\":\"As America\\u0027s Un-carrier, T-Mobile US, Inc. (NASDAQ: TMUS) is redefining the way consumers and\\nbusinesses buy wireless services through leading product and service innovation.\\nResponsibilities:\\n    *      Installed and Configured Hortonworks Data Platform (HDP) and Apache Ambari.\\n    *      Installed and Configured Hadoop Ecosystem (MapReduce, Pig, Sqoop. Hive, Kafka) both manually\\n     and using Ambari Server.\\n    *      Supported in setting up QA environment and updating configurations for implementing scripts\\n     with Pig and Sqoop. Worked on tuning the performance Pig queries.\\n    *      Converted ETL operations to Hadoop system using Pig Latin Operations, transformations and\\n     functions.\\n    *      Capturing data from existing databases that provide SQL interfaces using Sqoop.\\n    *      Worked on YARN capacity scheduler by creating queues to allocate resource guarantee to\\n     specific groups.\\n    *      Responsible for adding new eco system components, like spark, flume, Knox with required custom\\n     configurations based on the requirements.\\n    *      Installed and configured Kafka Cluster with additional zookeeper service.\\n    *      Helped the team to increase cluster size. The configuration for additional data nodes was\\n     managed using Puppet manifests.\\n    *      knowledge of open source system monitoring and event handling tools like Nagios.\\n    *      Integrated BI and Analytical tools like Tableau, Business Objects, and SAS with Hadoop\\n     Cluster.\\n    *      Worked with Ranger, Knox configuration to provide centralized security to Hadoop services.\\n    *      Expertise with NoSQL databases like Hbase and Cassandra.\\n    *      Planning and implementation of data migration from existing staging to production cluster.\\n     Even migrated data from existing databases to cloud (S3 and AWS RDS).\\n    *      Analyze escalated incidences within the Azure SQL database. Implemented test scripts to\\n     support test driven development and continuous integration.\\n    *      Developed Python and Shell/Perl Scripts for automation purpose.\\n    *      Troubleshooting experience in debugging and fixed the wrong data or data missing problem for\\n     Oracle Database.\\nEnvironment: Hadoop, Hdfs, Spark, MapReduce, Yarn, Pig, Hive, Sqoop, Kafka, Linux, Azure, HBase,\\nCassandra, MySQL, Kerberos, Python, Shell Scripting, Tableau, SAS.\",\"jobtype\":\"fulltime\",\"experienceitem\":\"Big Data Engineer TMobile, Bellevue, WA Sep 2014 - Dec 2015 As America\\u0027s Un-carrier, T-Mobile US, Inc. (NASDAQ: TMUS) is redefining the way consumers and businesses buy wireless services through leading product and service innovation. Responsibilities: * Installed and Configured Hortonworks Data Platform (HDP) and Apache Ambari. * Installed and Configured Hadoop Ecosystem (MapReduce, Pig, Sqoop. Hive, Kafka) both manually and using Ambari Server. * Supported in setting up QA environment and updating configurations for implementing scripts with Pig and Sqoop. Worked on tuning the performance Pig queries. * Converted ETL operations to Hadoop system using Pig Latin Operations, transformations and functions. * Capturing data from existing databases that provide SQL interfaces using Sqoop. * Worked on YARN capacity scheduler by creating queues to allocate resource guarantee to specific groups. * Responsible for adding new eco system components, like spark, flume, Knox with required custom configurations based on the requirements. * Installed and configured Kafka Cluster with additional zookeeper service. * Helped the team to increase cluster size. The configuration for additional data nodes was managed using Puppet manifests. * knowledge of open source system monitoring and event handling tools like Nagios. * Integrated BI and Analytical tools like Tableau, Business Objects, and SAS with Hadoop Cluster. * Worked with Ranger, Knox configuration to provide centralized security to Hadoop services. * Expertise with NoSQL databases like Hbase and Cassandra. * Planning and implementation of data migration from existing staging to production cluster. Even migrated data from existing databases to cloud (S3 and AWS RDS). * Analyze escalated incidences within the Azure SQL database. Implemented test scripts to support test driven development and continuous integration. * Developed Python and Shell/Perl Scripts for automation purpose. * Troubleshooting experience in debugging and fixed the wrong data or data missing problem for Oracle Database. Environment: Hadoop, Hdfs, Spark, MapReduce, Yarn, Pig, Hive, Sqoop, Kafka, Linux, Azure, HBase, Cassandra, MySQL, Kerberos, Python, Shell Scripting, Tableau, SAS.\",\"experiencemonths\":\"16\"},{\"experiencecity\":\"Chicago\",\"experienceregion\":\"IL\",\"experiencecountry_english\":\"United States\",\"experience\":\"Hadoop Engineer\",\"derived_profession_final\":\"hadoop engineer\",\"derived_profession_description\":\"Data Architect (m/f)\",\"derived_profession_code_id\":\"3126\",\"derived_profession_group_id\":\"79\",\"derived_profession_class_id\":\"9\",\"ansi_linked_experiencebegin_nowstring\":\"2013-06-01\",\"ansi_linked_experienceend_nowstring\":\"2014-08-31\",\"experienceorg\":\"Reply Inc.\",\"expdescriptionblock\":\"Reply is a company that specializes in consulting, system integration \\u0026 digital services, with a\\nfocus on the design and implementation of solutions based on the web and social networks. It\\nutilizes a network model consisting of companies operating in different sectors, including big data,\\ncloud computing, digital media and the internet of things.\\nResponsibilities:\\n    *      Installed/Configured Hadoop Map Reduce, HDFS and developed Map Reduce jobs in Java for data\\n     cleaning and processing.\\n    *      Responsible for troubleshooting issues in the execution of Map Reduce jobs by inspecting and\\n     reviewing log files.\\n    *      Implemented and Configured High Availability Hadoop Cluster (Quorum Based).\\n    *      Worked on Ingesting data into HDFS using Sqoop and Flume.\\n    *      Loaded/ Transformed large sets of structured, semi structured and unstructured data.\\n    *      Worked on different databases such as Oracle and MySql.\\n    *      Developed Oozie Workflows for daily incremental loads, which gets data from Teradata and then\\n     imported into hive tables.\\n    *      Used Cloudera manager to pull metrics on various cluster features like JVM, Running Map and\\n     reduce tasks.\\n    *      Developed PIG Latin scripts to extract the data from the web server output files to load into\\n     HDFS\\n    *      Performed upgrades and configuration changes, Commissioned/decommission nodes as needed to\\n     maintain load balancing and latency on the servers.\\n    *      Maintained DNS, NFS, and DHCP, printing, mail, web, and FTP services for the enterprise.\\n    *      Administered VMware and Virtual Box environment, evaluated and implemented new hardware and\\n     software.\\n    *      Worked on NGINX for proxy and reverse proxy rewrite rules.\\n    *      Worked on the Oracle databases in the backend to execute the DMLs and DDLs.\\nEnvironment: Hadoop, HDFS, MapReduce, Sqoop, Hive, Flume, MySQL, MR, HBase, PIG Latin, Sqoop, Chef,\\nOracle db,\",\"jobtype\":\"fulltime\",\"experienceitem\":\"Hadoop Engineer Reply Inc., Chicago, IL Jun 2013 - Aug 2014 Reply is a company that specializes in consulting, system integration \\u0026 digital services, with a focus on the design and implementation of solutions based on the web and social networks. It utilizes a network model consisting of companies operating in different sectors, including big data, cloud computing, digital media and the internet of things. Responsibilities: * Installed/Configured Hadoop Map Reduce, HDFS and developed Map Reduce jobs in Java for data cleaning and processing. * Responsible for troubleshooting issues in the execution of Map Reduce jobs by inspecting and reviewing log files. * Implemented and Configured High Availability Hadoop Cluster (Quorum Based). * Worked on Ingesting data into HDFS using Sqoop and Flume. * Loaded/ Transformed large sets of structured, semi structured and unstructured data. * Worked on different databases such as Oracle and MySql. * Developed Oozie Workflows for daily incremental loads, which gets data from Teradata and then imported into hive tables. * Used Cloudera manager to pull metrics on various cluster features like JVM, Running Map and reduce tasks. * Developed PIG Latin scripts to extract the data from the web server output files to load into HDFS * Performed upgrades and configuration changes, Commissioned/decommission nodes as needed to maintain load balancing and latency on the servers. * Maintained DNS, NFS, and DHCP, printing, mail, web, and FTP services for the enterprise. * Administered VMware and Virtual Box environment, evaluated and implemented new hardware and software. * Worked on NGINX for proxy and reverse proxy rewrite rules. * Worked on the Oracle databases in the backend to execute the DMLs and DDLs. Environment: Hadoop, HDFS, MapReduce, Sqoop, Hive, Flume, MySQL, MR, HBase, PIG Latin, Sqoop, Chef, Oracle db\",\"experiencemonths\":\"15\"},{\"experiencecity\":\"Austin\",\"experienceregion\":\"TX\",\"experiencecountry_english\":\"United States\",\"experience\":\"Big Data Analyst\",\"derived_profession_final\":\"big data analyst\",\"derived_profession_description\":\"Data Analyst (m/f)\",\"derived_profession_code_id\":\"2627\",\"derived_profession_group_id\":\"86\",\"derived_profession_class_id\":\"9\",\"ansi_linked_experiencebegin_nowstring\":\"2012-12-01\",\"ansi_linked_experienceend_nowstring\":\"2013-05-31\",\"experienceorg\":\"Applied Materials\",\"expdescriptionblock\":\"Applied Materials, Inc. is an American corporation that supplies equipment, services and software to\\nenable the manufacture of semiconductor (integrated circuit) chips for electronics, flat panel\\ndisplays for computers, smart phones and televisions, and solar products. In order to process their\\nhuge data, they started using Hadoop.\\nResponsibilities:\\n    *      Participated in the review of functional and non-functional requirements.\\n    *      Designed and implemented Map Reduce jobs to analyze the data collected by the Flume server.\\n    *      Worked with Hadoop Administration team to debug various slow running MR jobs and perform the\\n     necessary optimizations.\\n    *      Monitored cluster health status on daily basis, tuning system performance related\\n     configuration parameters, backing up configuration XML files.\\n    *      Worked with network and Linux system engineers/admin to define optimum network configurations,\\n     server hardware and operating system.\\n    *      Developed and implemented APIs to retrieve the data from Hadoop Platform to Web Application.\\n    *      Created Hive tables and worked on them using Hive QL.\\n    *      Performed Bug fixing of various modules that were raised by the Testing teams in the\\n     application during the Integration testing phase.\\n    *      Facilitated knowledge transfer sessions.\\nEnvironment: Hadoop, Cloudera CDH, Java, Eclipse, Pig, Hive, Ubuntu, Shell Scripting.\",\"jobtype\":\"fulltime\",\"experienceitem\":\"Big Data Analyst Applied Materials, Austin, TX Dec 2012 - May 2013 Applied Materials, Inc. is an American corporation that supplies equipment, services and software to enable the manufacture of semiconductor (integrated circuit) chips for electronics, flat panel displays for computers, smart phones and televisions, and solar products. In order to process their huge data, they started using Hadoop. Responsibilities: * Participated in the review of functional and non-functional requirements. * Designed and implemented Map Reduce jobs to analyze the data collected by the Flume server. * Worked with Hadoop Administration team to debug various slow running MR jobs and perform the necessary optimizations. * Monitored cluster health status on daily basis, tuning system performance related configuration parameters, backing up configuration XML files. * Worked with network and Linux system engineers/admin to define optimum network configurations, server hardware and operating system. * Developed and implemented APIs to retrieve the data from Hadoop Platform to Web Application. * Created Hive tables and worked on them using Hive QL. * Performed Bug fixing of various modules that were raised by the Testing teams in the application during the Integration testing phase. * Facilitated knowledge transfer sessions. Environment: Hadoop, Cloudera CDH, Java, Eclipse, Pig, Hive, Ubuntu, Shell Scripting.\",\"experiencemonths\":\"6\"},{\"experiencecountry_english\":\"India\",\"experience\":\"System Analyst/Admin\",\"derived_profession_final\":\"system analyst admin\",\"derived_profession_description\":\"Systems Analyst (m/f)\",\"derived_profession_code_id\":\"2625\",\"derived_profession_group_id\":\"86\",\"derived_profession_class_id\":\"9\",\"ansi_linked_experiencebegin_nowstring\":\"2011-12-01\",\"ansi_linked_experienceend_nowstring\":\"2012-11-30\",\"experienceorg\":\"Dwaith Infotech\",\"expdescriptionblock\":\"Dwaith Infotech, a leading IT Outsourcing, IT Consulting firm offering managed IT services,\\nEnterprise Business Solutions and Collaborative client partnerships through its flexible Global\\nDelivery model. Dwaith focus on select industries and service lines, enabling us to deliver the best\\npossible solutions to meet our clients\\u0027 needs.\\nResponsibilities:\\n    *      Have been responsible for administering large, multi-site UNIX/LINUX server environments and\\n     operating systems, software installation, upgrades, system integrity, security, disaster\\n     recovery and performance.\\n    *      Implemented Cloudera Hadoop clusters for three different environments on HPE ProLiant servers.\\n    *      Configured/Managed Cisco and Brocade Fabric environment for soft/hard Zoning.\\n    *      Worked on MySQL and successfully launched queries to provide required data to the department.\\n    *      Created users, manage user permissions, maintain User \\u0026 File System quota on Redhat Linux.\\n    *      Installation \\u0026 Configuration of Logical Volume Manager - LVM and RAID.\\n    *      Automated administration tasks through use of scripting and Job Scheduling using CRON.\\n    *      Wrote shell scripts for taking data backups, cleaning junk content and updating software\\n     regularly.\\n    *      Experience in using protocols/services like Http, Https, TLI/SSL, DHCP, DNS, SSH, SFTP,\\n     TCP/IP, FTP/SFTP, SMTP\\n    *      Provided Linux System Administration, Linux System Security, Project Management and Risk\\n     Management in Information Systems.\\n    *      Day to day provisioning of storage including (Storage device/LUN/Volume selection \\u0026 creation,\\n     Fabric Zoning, LUN Masking \\u0026 Mapping).\\n    *      Administration of environment running VMware ESXi Hosts and Virtual Machines.\\n    *      Worked successfully towards improving and maintaining the Backup Success rate to \\u003e 98%.\\n    *      Worked with server teams to insure the configuration and installation of the proper drivers,\\n     firmware, and multipath drivers to support the SAN environment.\\n    *      Provide performance tuning and regular maintenance in order to minimize downtime and maximize\\n     performance.\\n    *      Take care about Data Center (DC) by ordering and upgrading necessary hardware, supporting\\n     RAIDs, maintaining servers and installing new ones.\\nEnvironment: Linux, VMware, Storage\",\"jobtype\":\"fulltime\",\"experienceitem\":\"System Analyst/Admin Dwaith Infotech, India Dec 2011 - Nov 2012 Dwaith Infotech, a leading IT Outsourcing, IT Consulting firm offering managed IT services, Enterprise Business Solutions and Collaborative client partnerships through its flexible Global Delivery model. Dwaith focus on select industries and service lines, enabling us to deliver the best possible solutions to meet our clients\\u0027 needs. Responsibilities: * Have been responsible for administering large, multi-site UNIX/LINUX server environments and operating systems, software installation, upgrades, system integrity, security, disaster recovery and performance. * Implemented Cloudera Hadoop clusters for three different environments on HPE ProLiant servers. * Configured/Managed Cisco and Brocade Fabric environment for soft/hard Zoning. * Worked on MySQL and successfully launched queries to provide required data to the department. * Created users, manage user permissions, maintain User \\u0026 File System quota on Redhat Linux. * Installation \\u0026 Configuration of Logical Volume Manager - LVM and RAID. * Automated administration tasks through use of scripting and Job Scheduling using CRON. * Wrote shell scripts for taking data backups, cleaning junk content and updating software regularly. * Experience in using protocols/services like Http, Https, TLI/SSL, DHCP, DNS, SSH, SFTP, TCP/IP, FTP/SFTP, SMTP * Provided Linux System Administration, Linux System Security, Project Management and Risk Management in Information Systems. * Day to day provisioning of storage including (Storage device/LUN/Volume selection \\u0026 creation, Fabric Zoning, LUN Masking \\u0026 Mapping). * Administration of environment running VMware ESXi Hosts and Virtual Machines. * Worked successfully towards improving and maintaining the Backup Success rate to \\u003e 98%. * Worked with server teams to insure the configuration and installation of the proper drivers, firmware, and multipath drivers to support the SAN environment. * Provide performance tuning and regular maintenance in order to minimize downtime and maximize performance. * Take care about Data Center (DC) by ordering and upgrading necessary hardware, supporting RAIDs, maintaining servers and installing new ones. Environment: Linux, VMware, Storage\",\"experiencemonths\":\"12\"}],\"educationitem\":[{\"degreedirection\":\"Bachelor of Technology in Computer Science\",\"degreemajor\":\"Computer Science\",\"educationitem\":\"* Bachelor of Technology in Computer Science.\",\"degree_international\":\"3\"}],\"totalexperiencemonths\":[{\"totalexperiencemonths\":\"94\"}],\"highesteducationitem\":[{\"degreedirection\":\"Bachelor of Technology in Computer Science\",\"degreemajor\":\"Computer Science\",\"degreetype\":\"Bachelor\\u0027s degree\",\"educationitem\":\"* Bachelor of Technology in Computer Science.\",\"degree_international\":\"3\"}],\"lastitemwithjobtitle\":[{\"experiencecity\":\"Cincinnati\",\"experienceregion\":\"OH\",\"experiencecountry_english\":\"United States\",\"experience\":\"Hadoop Administrator\",\"derived_profession_final\":\"hadoop administrator\",\"derived_profession_description\":\"Data Architect (m/f)\",\"derived_profession_code_id\":\"3126\",\"derived_profession_group_id\":\"79\",\"derived_profession_class_id\":\"9\",\"ansi_linked_experiencebegin_nowstring\":\"1984-05-01\",\"expdescriptionblock\":\"84.51 is a consumer insights subsidiary to Kroger. It is majorly a data science analytics company\\nwhose applications help in building strong relationship between people and brands.\\nResponsibilities:\\n    *      Administering and managing large-scale Hadoop clusters (6 Pb Storage) and also part of\\n     capacity planning team for optimizing/scaling the clusters for future needs.\\n    *      Worked on different Hadoop clusters running on Cloudera and Hortonworks distributions.\\n    *      Responsible for day-to-day activities which include Hadoop support, Cluster maintenance,\\n     handling nodes, and Monitoring/ Troubleshooting clusters.\\n    *      Supported and configured Pyspark jobs through Jupyter sessions and R scripts through RStudio.\\n    *      Installed various python and R packages.\\n    *      Implemented high availability, DR strategies, load balancers for all the Hadoop services along\\n     with Cloudera manager to overcome single point of failure.\\n    *      Worked with fellow developers in compressing the data, modifying Spark application code with\\n     repartition, coalesce methods in order to reduce the small files and storage on cluster side.\\n    *      Installed and configured Unravel (Application Performance Management tool) to optimize the\\n     resources and run the applications effectively.\\n    *      Used/Configured Nifi Flows on Hortonworks platform for data ingestion purposes and also to\\n     export data from hdfs to Exadata.\\n    *      Worked on setting up Kafka to integrate with Nifi to ingest the live streaming data.\\n    *      Worked with Cloud Team (GCP and later Azure) to migrate hdfs data into cloud.\\n    *      Automated various Day to Day works using shell and python scripts.\\n    *      Oozie workflows, Airflow jobs, shell/python scripts were setup for effective Cluster\\n     Monitoring and to run applications.\\n    *      Integrated both Hadoop clusters and Cloud Infrastructure with Grafana and Dynatrace for\\n     performance monitoring.\\n    *      Secured the data/user access with ACL\\u0027s, Kerberos, Sentry and Encryption Zones.\\nEnvironment: Hadoop, Hdfs, Spark, MapReduce, Yarn, Hive, Sqoop, Oozie, Airflow, Nifi, Kafka,\\nKerberos, Shell Scripting, Unravel.\",\"experienceitem\":\"Hadoop Administrator 84.51, Cincinnati, OH Feb 2019 - Present 84.51 is a consumer insights subsidiary to Kroger. It is majorly a data science analytics company whose applications help in building strong relationship between people and brands. Responsibilities: * Administering and managing large-scale Hadoop clusters (6 Pb Storage) and also part of capacity planning team for optimizing/scaling the clusters for future needs. * Worked on different Hadoop clusters running on Cloudera and Hortonworks distributions. * Responsible for day-to-day activities which include Hadoop support, Cluster maintenance, handling nodes, and Monitoring/ Troubleshooting clusters. * Supported and configured Pyspark jobs through Jupyter sessions and R scripts through RStudio. * Installed various python and R packages. * Implemented high availability, DR strategies, load balancers for all the Hadoop services along with Cloudera manager to overcome single point of failure. * Worked with fellow developers in compressing the data, modifying Spark application code with repartition, coalesce methods in order to reduce the small files and storage on cluster side. * Installed and configured Unravel (Application Performance Management tool) to optimize the resources and run the applications effectively. * Used/Configured Nifi Flows on Hortonworks platform for data ingestion purposes and also to export data from hdfs to Exadata. * Worked on setting up Kafka to integrate with Nifi to ingest the live streaming data. * Worked with Cloud Team (GCP and later Azure) to migrate hdfs data into cloud. * Automated various Day to Day works using shell and python scripts. * Oozie workflows, Airflow jobs, shell/python scripts were setup for effective Cluster Monitoring and to run applications. * Integrated both Hadoop clusters and Cloud Infrastructure with Grafana and Dynatrace for performance monitoring. * Secured the data/user access with ACL\\u0027s, Kerberos, Sentry and Encryption Zones. Environment: Hadoop, Hdfs, Spark, MapReduce, Yarn, Hive, Sqoop, Oozie, Airflow, Nifi, Kafka, Kerberos, Shell Scripting, Unravel.\",\"experiencemonths\":\"8\"}],\"validated_compskill\":[{\"ansi_linked_first_used_nowstring\":\"2013-06-01\",\"ansi_linked_last_used_nowstring\":\"2019-01-31\",\"duration_derived_months\":\"68\",\"validated_compskill\":\"HBase\"},{\"ansi_linked_first_used_nowstring\":\"1984-05-01\",\"ansi_linked_last_used_nowstring\":\"2019-01-31\",\"duration_derived_months\":\"61\",\"validated_compskill\":\"Spark\"},{\"validated_compskill\":\"Ranger\"},{\"validated_compskill\":\"IAM\"},{\"ansi_linked_first_used_nowstring\":\"2016-01-01\",\"ansi_linked_last_used_nowstring\":\"2017-09-30\",\"duration_derived_months\":\"21\",\"validated_compskill\":\"VPC\"},{\"ansi_linked_first_used_nowstring\":\"2013-06-01\",\"ansi_linked_last_used_nowstring\":\"2014-08-31\",\"duration_derived_months\":\"15\",\"validated_compskill\":\"Nginx\"},{\"validated_compskill\":\"Virtual Box\"},{\"ansi_linked_first_used_nowstring\":\"1984-05-01\",\"ansi_linked_last_used_nowstring\":\"2019-01-31\",\"duration_derived_months\":\"76\",\"validated_compskill\":\"MapReduce\"},{\"ansi_linked_first_used_nowstring\":\"2011-12-01\",\"ansi_linked_last_used_nowstring\":\"2014-08-31\",\"duration_derived_months\":\"27\",\"validated_compskill\":\"VMware\"},{\"validated_compskill\":\"WebSphere\"},{\"ansi_linked_first_used_nowstring\":\"1984-05-01\",\"duration_derived_months\":\"8\",\"validated_compskill\":\"Pyspark\"},{\"ansi_linked_first_used_nowstring\":\"1984-05-01\",\"duration_derived_months\":\"8\",\"validated_compskill\":\"Airflow\"},{\"ansi_linked_first_used_nowstring\":\"2013-06-01\",\"ansi_linked_last_used_nowstring\":\"2017-09-30\",\"duration_derived_months\":\"36\",\"validated_compskill\":\"log files\"},{\"ansi_linked_first_used_nowstring\":\"2016-01-01\",\"ansi_linked_last_used_nowstring\":\"2017-09-30\",\"duration_derived_months\":\"21\",\"validated_compskill\":\"AWS EC2\"},{\"ansi_linked_first_used_nowstring\":\"2016-01-01\",\"ansi_linked_last_used_nowstring\":\"2017-09-30\",\"duration_derived_months\":\"21\",\"validated_compskill\":\"RHEL\"},{\"ansi_linked_first_used_nowstring\":\"2014-09-01\",\"ansi_linked_last_used_nowstring\":\"2015-12-31\",\"duration_derived_months\":\"16\",\"validated_compskill\":\"Business Objects\"},{\"ansi_linked_first_used_nowstring\":\"2014-09-01\",\"ansi_linked_last_used_nowstring\":\"2015-12-31\",\"duration_derived_months\":\"16\",\"validated_compskill\":\"test scripts\"},{\"ansi_linked_first_used_nowstring\":\"2014-09-01\",\"ansi_linked_last_used_nowstring\":\"2015-12-31\",\"duration_derived_months\":\"16\",\"validated_compskill\":\"debugging\"},{\"ansi_linked_first_used_nowstring\":\"2012-12-01\",\"ansi_linked_last_used_nowstring\":\"2013-05-31\",\"duration_derived_months\":\"6\",\"validated_compskill\":\"backing up\"},{\"ansi_linked_first_used_nowstring\":\"2011-12-01\",\"ansi_linked_last_used_nowstring\":\"2012-11-30\",\"duration_derived_months\":\"12\",\"validated_compskill\":\"Job Scheduling\"}],\"softskill\":[{\"softskill\":\"managing\"},{\"softskill\":\"able to work under pressure\"},{\"softskill\":\"Excellent communicative\"},{\"softskill\":\"interpersonal\"},{\"softskill\":\"intuitive\"},{\"softskill\":\"analysis\"},{\"softskill\":\"leadership skills\"},{\"softskill\":\"ability to work efficiently in both independent and team work environments\"}],\"langskill\":[{\"langskill\":\"English\",\"langskill_code\":\"EN\"}],\"language\":[{\"language\":\"english\"}]}","service":"TK_RESUME","insertionTime":1643774408741}